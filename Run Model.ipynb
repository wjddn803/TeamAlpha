{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/fb/863012b13912709c13cf5cfdbfb304fa6c727659d6290438e1a88df9d848/pip-19.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.0.3\n",
      "    Uninstalling pip-19.0.3:\n",
      "      Successfully uninstalled pip-19.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\JUNGWO~1\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-jme9_aly\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyrouge\n",
      "  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
      "Building wheels for collected packages: pyrouge\n",
      "  Building wheel for pyrouge (setup.py): started\n",
      "  Building wheel for pyrouge (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Jungwoo Lim\\AppData\\Local\\pip\\Cache\\wheels\\75\\d3\\0c\\e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
      "Successfully built pyrouge\n",
      "Installing collected packages: pyrouge\n",
      "Successfully installed pyrouge-0.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jungwoo Lim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Author: Shashi Narayan\n",
    "# Date: September 2016\n",
    "# Project: Document Summarization\n",
    "# H2020 Summa Project\n",
    "# Comments: Jan 2017\n",
    "# Improved for Reinforcement Learning\n",
    "####################################\n",
    "\n",
    "\"\"\"\n",
    "Document Summarization System\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from reward_utils import Reward_Generator\n",
    "from data_utils import DataProcessor\n",
    "from my_flags import FLAGS\n",
    "from my_model import MY_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare vocab dict and read pretrained word embeddings ...\n",
      "Reading pretrained word embeddings file: C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\data\\1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\to\\training\\directory\\vocab.txt\n",
      "Prepare training data ...\n",
      "Data file prefix (.doc, .title, .image, .label.multipleoracle): C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\data\\preprocessed-input-directory/cnn.training\n",
      "Data sizes: 90262 90262 90262 90262\n",
      "Reading data (no padding to save memory) ...\n",
      "0 ...\n",
      "10000 ...\n",
      "20000 ...\n",
      "30000 ...\n",
      "40000 ...\n",
      "50000 ...\n",
      "60000 ...\n",
      "70000 ...\n",
      "80000 ...\n",
      "90000 ...\n",
      "Writing data files with prefix (.filename, .doc, .title, .image, .label, .weight, .rewards): C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\to\\training\\directory/cnn.training\n",
      "Prepare validation data ...\n",
      "Data file prefix (.doc, .title, .image, .label.multipleoracle): C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\data\\preprocessed-input-directory/cnn.validation\n",
      "Data sizes: 1220 1220 1220 1220\n",
      "Reading data (no padding to save memory) ...\n",
      "0 ...\n",
      "Writing data files with prefix (.filename, .doc, .title, .image, .label, .weight, .rewards): C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\address\\to\\training\\directory/cnn.validation\n",
      "Prepare ROUGE reward generator ...\n",
      "WARNING:tensorflow:From C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\model_docsum.py:501: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jungwoo Lim\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Jungwoo Lim\\Documents\\GitHub\\Refresh\\my_model.py:73: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Jungwoo Lim\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialize word embedding vocabulary with pretrained embeddings ...\n",
      "Start Reinforcement Training (single rollout from largest prob mass) ...\n",
      "MRT: Epoch 1\n",
      "MRT: Epoch 1 : Reshuffle training document indices\n",
      "MRT: Epoch 1 : Restore Rouge Dict\n",
      "MRT: Epoch 1 : Covered 20/90262 : Minibatch Reward Weighted Multisample CE Loss= 352.051178 : Minibatch training accuracy= 0.105213\n",
      "MRT: Epoch 1 : Covered 40/90262 : Minibatch Reward Weighted Multisample CE Loss= 210.303467 : Minibatch training accuracy= 0.909218\n",
      "MRT: Epoch 1 : Covered 60/90262 : Minibatch Reward Weighted Multisample CE Loss= 131.024689 : Minibatch training accuracy= 0.912421\n",
      "MRT: Epoch 1 : Covered 80/90262 : Minibatch Reward Weighted Multisample CE Loss= 66.489883 : Minibatch training accuracy= 0.886724\n",
      "MRT: Epoch 1 : Covered 100/90262 : Minibatch Reward Weighted Multisample CE Loss= 24.675186 : Minibatch training accuracy= 0.893225\n",
      "MRT: Epoch 1 : Covered 120/90262 : Minibatch Reward Weighted Multisample CE Loss= 14.220901 : Minibatch training accuracy= 0.681031\n",
      "MRT: Epoch 1 : Covered 140/90262 : Minibatch Reward Weighted Multisample CE Loss= 11.709589 : Minibatch training accuracy= 0.782530\n",
      "MRT: Epoch 1 : Covered 160/90262 : Minibatch Reward Weighted Multisample CE Loss= 8.695297 : Minibatch training accuracy= 0.873784\n",
      "MRT: Epoch 1 : Covered 180/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.790810 : Minibatch training accuracy= 0.884257\n",
      "MRT: Epoch 1 : Covered 200/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.436244 : Minibatch training accuracy= 0.880379\n",
      "MRT: Epoch 1 : Covered 220/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.054095 : Minibatch training accuracy= 0.822608\n",
      "MRT: Epoch 1 : Covered 240/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.046014 : Minibatch training accuracy= 0.872832\n",
      "MRT: Epoch 1 : Covered 260/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.215472 : Minibatch training accuracy= 0.852345\n",
      "MRT: Epoch 1 : Covered 280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.709109 : Minibatch training accuracy= 0.905389\n",
      "MRT: Epoch 1 : Covered 300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.519450 : Minibatch training accuracy= 0.844129\n",
      "MRT: Epoch 1 : Covered 320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.717384 : Minibatch training accuracy= 0.901953\n",
      "MRT: Epoch 1 : Covered 340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.348508 : Minibatch training accuracy= 0.887352\n",
      "MRT: Epoch 1 : Covered 360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.138052 : Minibatch training accuracy= 0.883987\n",
      "MRT: Epoch 1 : Covered 380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.588307 : Minibatch training accuracy= 0.896223\n",
      "MRT: Epoch 1 : Covered 400/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.360450 : Minibatch training accuracy= 0.892813\n",
      "MRT: Epoch 1 : Covered 420/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.122922 : Minibatch training accuracy= 0.893019\n",
      "MRT: Epoch 1 : Covered 440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.987310 : Minibatch training accuracy= 0.860559\n",
      "MRT: Epoch 1 : Covered 460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.617942 : Minibatch training accuracy= 0.908799\n",
      "MRT: Epoch 1 : Covered 480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.291714 : Minibatch training accuracy= 0.881526\n",
      "MRT: Epoch 1 : Covered 500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.659873 : Minibatch training accuracy= 0.892435\n",
      "MRT: Epoch 1 : Covered 520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.763086 : Minibatch training accuracy= 0.911712\n",
      "MRT: Epoch 1 : Covered 540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.375396 : Minibatch training accuracy= 0.894561\n",
      "MRT: Epoch 1 : Covered 560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.218454 : Minibatch training accuracy= 0.883152\n",
      "MRT: Epoch 1 : Covered 580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.833678 : Minibatch training accuracy= 0.889147\n",
      "MRT: Epoch 1 : Covered 600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.191756 : Minibatch training accuracy= 0.855658\n",
      "MRT: Epoch 1 : Covered 620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.517971 : Minibatch training accuracy= 0.845424\n",
      "MRT: Epoch 1 : Covered 640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.373316 : Minibatch training accuracy= 0.897967\n",
      "MRT: Epoch 1 : Covered 660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.976100 : Minibatch training accuracy= 0.895401\n",
      "MRT: Epoch 1 : Covered 680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.339069 : Minibatch training accuracy= 0.851092\n",
      "MRT: Epoch 1 : Covered 700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.671686 : Minibatch training accuracy= 0.847407\n",
      "MRT: Epoch 1 : Covered 720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.617912 : Minibatch training accuracy= 0.890463\n",
      "MRT: Epoch 1 : Covered 740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.186428 : Minibatch training accuracy= 0.862961\n",
      "MRT: Epoch 1 : Covered 760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.460014 : Minibatch training accuracy= 0.889310\n",
      "MRT: Epoch 1 : Covered 780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.803116 : Minibatch training accuracy= 0.896044\n",
      "MRT: Epoch 1 : Covered 800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.600538 : Minibatch training accuracy= 0.887467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.060068 : Minibatch training accuracy= 0.896907\n",
      "MRT: Epoch 1 : Covered 840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.245074 : Minibatch training accuracy= 0.873516\n",
      "MRT: Epoch 1 : Covered 860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.315844 : Minibatch training accuracy= 0.905887\n",
      "MRT: Epoch 1 : Covered 880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.106751 : Minibatch training accuracy= 0.876988\n",
      "MRT: Epoch 1 : Covered 900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.505981 : Minibatch training accuracy= 0.885550\n",
      "MRT: Epoch 1 : Covered 920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.924567 : Minibatch training accuracy= 0.892563\n",
      "MRT: Epoch 1 : Covered 940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.121534 : Minibatch training accuracy= 0.879473\n",
      "MRT: Epoch 1 : Covered 960/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.198567 : Minibatch training accuracy= 0.872878\n",
      "MRT: Epoch 1 : Covered 980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.435647 : Minibatch training accuracy= 0.906830\n",
      "MRT: Epoch 1 : Covered 1000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.981194 : Minibatch training accuracy= 0.901169\n",
      "MRT: Epoch 1 : Covered 1020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.293114 : Minibatch training accuracy= 0.877003\n",
      "MRT: Epoch 1 : Covered 1040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.640618 : Minibatch training accuracy= 0.886614\n",
      "MRT: Epoch 1 : Covered 1060/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.033537 : Minibatch training accuracy= 0.913691\n",
      "MRT: Epoch 1 : Covered 1080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.495917 : Minibatch training accuracy= 0.893997\n",
      "MRT: Epoch 1 : Covered 1100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.198073 : Minibatch training accuracy= 0.918258\n",
      "MRT: Epoch 1 : Covered 1120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.150594 : Minibatch training accuracy= 0.928270\n",
      "MRT: Epoch 1 : Covered 1140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.734839 : Minibatch training accuracy= 0.917373\n",
      "MRT: Epoch 1 : Covered 1160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.452244 : Minibatch training accuracy= 0.897639\n",
      "MRT: Epoch 1 : Covered 1180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.494471 : Minibatch training accuracy= 0.895448\n",
      "MRT: Epoch 1 : Covered 1200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.705725 : Minibatch training accuracy= 0.908083\n",
      "MRT: Epoch 1 : Covered 1220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.042457 : Minibatch training accuracy= 0.910594\n",
      "MRT: Epoch 1 : Covered 1240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.474905 : Minibatch training accuracy= 0.901712\n",
      "MRT: Epoch 1 : Covered 1260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.393243 : Minibatch training accuracy= 0.891867\n",
      "MRT: Epoch 1 : Covered 1280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.700184 : Minibatch training accuracy= 0.911017\n",
      "MRT: Epoch 1 : Covered 1300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.174973 : Minibatch training accuracy= 0.895632\n",
      "MRT: Epoch 1 : Covered 1320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.361347 : Minibatch training accuracy= 0.896749\n",
      "MRT: Epoch 1 : Covered 1340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.238065 : Minibatch training accuracy= 0.919542\n",
      "MRT: Epoch 1 : Covered 1360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.195144 : Minibatch training accuracy= 0.896292\n",
      "MRT: Epoch 1 : Covered 1380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.170805 : Minibatch training accuracy= 0.896440\n",
      "MRT: Epoch 1 : Covered 1400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.433208 : Minibatch training accuracy= 0.889016\n",
      "MRT: Epoch 1 : Covered 1420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.054815 : Minibatch training accuracy= 0.900105\n",
      "MRT: Epoch 1 : Covered 1440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.157171 : Minibatch training accuracy= 0.885191\n",
      "MRT: Epoch 1 : Covered 1460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.888965 : Minibatch training accuracy= 0.890654\n",
      "MRT: Epoch 1 : Covered 1480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.147606 : Minibatch training accuracy= 0.909176\n",
      "MRT: Epoch 1 : Covered 1500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.748667 : Minibatch training accuracy= 0.852788\n",
      "MRT: Epoch 1 : Covered 1520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.252268 : Minibatch training accuracy= 0.837850\n",
      "MRT: Epoch 1 : Covered 1540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.339212 : Minibatch training accuracy= 0.885621\n",
      "MRT: Epoch 1 : Covered 1560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.274776 : Minibatch training accuracy= 0.878231\n",
      "MRT: Epoch 1 : Covered 1580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.274279 : Minibatch training accuracy= 0.909985\n",
      "MRT: Epoch 1 : Covered 1600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.235196 : Minibatch training accuracy= 0.884195\n",
      "MRT: Epoch 1 : Covered 1620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.195001 : Minibatch training accuracy= 0.891988\n",
      "MRT: Epoch 1 : Covered 1640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.412574 : Minibatch training accuracy= 0.908047\n",
      "MRT: Epoch 1 : Covered 1660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.196867 : Minibatch training accuracy= 0.903399\n",
      "MRT: Epoch 1 : Covered 1680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.192178 : Minibatch training accuracy= 0.879026\n",
      "MRT: Epoch 1 : Covered 1700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.043024 : Minibatch training accuracy= 0.815085\n",
      "MRT: Epoch 1 : Covered 1720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.262135 : Minibatch training accuracy= 0.878459\n",
      "MRT: Epoch 1 : Covered 1740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.946404 : Minibatch training accuracy= 0.868614\n",
      "MRT: Epoch 1 : Covered 1760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.146813 : Minibatch training accuracy= 0.898321\n",
      "MRT: Epoch 1 : Covered 1780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.952237 : Minibatch training accuracy= 0.867630\n",
      "MRT: Epoch 1 : Covered 1800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.057162 : Minibatch training accuracy= 0.881171\n",
      "MRT: Epoch 1 : Covered 1820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.826608 : Minibatch training accuracy= 0.881877\n",
      "MRT: Epoch 1 : Covered 1840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.905938 : Minibatch training accuracy= 0.910200\n",
      "MRT: Epoch 1 : Covered 1860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.009940 : Minibatch training accuracy= 0.888491\n",
      "MRT: Epoch 1 : Covered 1880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.278760 : Minibatch training accuracy= 0.874727\n",
      "MRT: Epoch 1 : Covered 1900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.700609 : Minibatch training accuracy= 0.907303\n",
      "MRT: Epoch 1 : Covered 1920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.027467 : Minibatch training accuracy= 0.927024\n",
      "MRT: Epoch 1 : Covered 1940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.232465 : Minibatch training accuracy= 0.875068\n",
      "MRT: Epoch 1 : Covered 1960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.430234 : Minibatch training accuracy= 0.904028\n",
      "MRT: Epoch 1 : Covered 1980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.511828 : Minibatch training accuracy= 0.888134\n",
      "MRT: Epoch 1 : Covered 2000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.130135 : Minibatch training accuracy= 0.893871\n",
      "MRT: Epoch 1 : Covered 2020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.508442 : Minibatch training accuracy= 0.908032\n",
      "MRT: Epoch 1 : Covered 2040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.296571 : Minibatch training accuracy= 0.858278\n",
      "MRT: Epoch 1 : Covered 2060/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.957179 : Minibatch training accuracy= 0.896058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 2080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.984259 : Minibatch training accuracy= 0.850583\n",
      "MRT: Epoch 1 : Covered 2100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.404133 : Minibatch training accuracy= 0.923166\n",
      "MRT: Epoch 1 : Covered 2120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.713287 : Minibatch training accuracy= 0.893626\n",
      "MRT: Epoch 1 : Covered 2140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.095722 : Minibatch training accuracy= 0.868665\n",
      "MRT: Epoch 1 : Covered 2160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.508728 : Minibatch training accuracy= 0.903095\n",
      "MRT: Epoch 1 : Covered 2180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.661273 : Minibatch training accuracy= 0.894910\n",
      "MRT: Epoch 1 : Covered 2200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.302888 : Minibatch training accuracy= 0.908405\n",
      "MRT: Epoch 1 : Covered 2220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.181622 : Minibatch training accuracy= 0.868458\n",
      "MRT: Epoch 1 : Covered 2240/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.871312 : Minibatch training accuracy= 0.904408\n",
      "MRT: Epoch 1 : Covered 2260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.430575 : Minibatch training accuracy= 0.880279\n",
      "MRT: Epoch 1 : Covered 2280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.580074 : Minibatch training accuracy= 0.862092\n",
      "MRT: Epoch 1 : Covered 2300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.176923 : Minibatch training accuracy= 0.855984\n",
      "MRT: Epoch 1 : Covered 2320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.932393 : Minibatch training accuracy= 0.875796\n",
      "MRT: Epoch 1 : Covered 2340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.294692 : Minibatch training accuracy= 0.898827\n",
      "MRT: Epoch 1 : Covered 2360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.060161 : Minibatch training accuracy= 0.886215\n",
      "MRT: Epoch 1 : Covered 2380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.523816 : Minibatch training accuracy= 0.916518\n",
      "MRT: Epoch 1 : Covered 2400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.368801 : Minibatch training accuracy= 0.883372\n",
      "MRT: Epoch 1 : Covered 2420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.531149 : Minibatch training accuracy= 0.921057\n",
      "MRT: Epoch 1 : Covered 2440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.127101 : Minibatch training accuracy= 0.899203\n",
      "MRT: Epoch 1 : Covered 2460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.749233 : Minibatch training accuracy= 0.889901\n",
      "MRT: Epoch 1 : Covered 2480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.441108 : Minibatch training accuracy= 0.901477\n",
      "MRT: Epoch 1 : Covered 2500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.374644 : Minibatch training accuracy= 0.897475\n",
      "MRT: Epoch 1 : Covered 2520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.322463 : Minibatch training accuracy= 0.890489\n",
      "MRT: Epoch 1 : Covered 2540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.387906 : Minibatch training accuracy= 0.869469\n",
      "MRT: Epoch 1 : Covered 2560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.423265 : Minibatch training accuracy= 0.904431\n",
      "MRT: Epoch 1 : Covered 2580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.412793 : Minibatch training accuracy= 0.916097\n",
      "MRT: Epoch 1 : Covered 2600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.386697 : Minibatch training accuracy= 0.899280\n",
      "MRT: Epoch 1 : Covered 2620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.130690 : Minibatch training accuracy= 0.880774\n",
      "MRT: Epoch 1 : Covered 2640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.952145 : Minibatch training accuracy= 0.892025\n",
      "MRT: Epoch 1 : Covered 2660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.314853 : Minibatch training accuracy= 0.902405\n",
      "MRT: Epoch 1 : Covered 2680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.807631 : Minibatch training accuracy= 0.878139\n",
      "MRT: Epoch 1 : Covered 2700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.271006 : Minibatch training accuracy= 0.886916\n",
      "MRT: Epoch 1 : Covered 2720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.429494 : Minibatch training accuracy= 0.900670\n",
      "MRT: Epoch 1 : Covered 2740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.949683 : Minibatch training accuracy= 0.901357\n",
      "MRT: Epoch 1 : Covered 2760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.308052 : Minibatch training accuracy= 0.854612\n",
      "MRT: Epoch 1 : Covered 2780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.448641 : Minibatch training accuracy= 0.896112\n",
      "MRT: Epoch 1 : Covered 2800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.307830 : Minibatch training accuracy= 0.888999\n",
      "MRT: Epoch 1 : Covered 2820/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.956270 : Minibatch training accuracy= 0.884046\n",
      "MRT: Epoch 1 : Covered 2840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.214463 : Minibatch training accuracy= 0.889946\n",
      "MRT: Epoch 1 : Covered 2860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.228943 : Minibatch training accuracy= 0.908425\n",
      "MRT: Epoch 1 : Covered 2880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.225059 : Minibatch training accuracy= 0.867952\n",
      "MRT: Epoch 1 : Covered 2900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.221021 : Minibatch training accuracy= 0.902721\n",
      "MRT: Epoch 1 : Covered 2920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.026345 : Minibatch training accuracy= 0.839529\n",
      "MRT: Epoch 1 : Covered 2940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.188673 : Minibatch training accuracy= 0.896813\n",
      "MRT: Epoch 1 : Covered 2960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.905746 : Minibatch training accuracy= 0.895203\n",
      "MRT: Epoch 1 : Covered 2980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.193646 : Minibatch training accuracy= 0.887429\n",
      "MRT: Epoch 1 : Covered 3000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.660406 : Minibatch training accuracy= 0.887340\n",
      "MRT: Epoch 1 : Covered 3020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.417805 : Minibatch training accuracy= 0.866928\n",
      "MRT: Epoch 1 : Covered 3040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.065976 : Minibatch training accuracy= 0.855216\n",
      "MRT: Epoch 1 : Covered 3060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.156267 : Minibatch training accuracy= 0.870023\n",
      "MRT: Epoch 1 : Covered 3080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.330989 : Minibatch training accuracy= 0.898633\n",
      "MRT: Epoch 1 : Covered 3100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.217126 : Minibatch training accuracy= 0.903741\n",
      "MRT: Epoch 1 : Covered 3120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.353803 : Minibatch training accuracy= 0.900548\n",
      "MRT: Epoch 1 : Covered 3140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.796855 : Minibatch training accuracy= 0.858080\n",
      "MRT: Epoch 1 : Covered 3160/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.990172 : Minibatch training accuracy= 0.858511\n",
      "MRT: Epoch 1 : Covered 3180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.335733 : Minibatch training accuracy= 0.889177\n",
      "MRT: Epoch 1 : Covered 3200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.430712 : Minibatch training accuracy= 0.887851\n",
      "MRT: Epoch 1 : Covered 3220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.392596 : Minibatch training accuracy= 0.892669\n",
      "MRT: Epoch 1 : Covered 3240/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.953634 : Minibatch training accuracy= 0.879753\n",
      "MRT: Epoch 1 : Covered 3260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.156468 : Minibatch training accuracy= 0.891156\n",
      "MRT: Epoch 1 : Covered 3280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.768526 : Minibatch training accuracy= 0.911177\n",
      "MRT: Epoch 1 : Covered 3300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.539268 : Minibatch training accuracy= 0.883064\n",
      "MRT: Epoch 1 : Covered 3320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.500622 : Minibatch training accuracy= 0.887001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 3340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.569903 : Minibatch training accuracy= 0.894344\n",
      "MRT: Epoch 1 : Covered 3360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.248807 : Minibatch training accuracy= 0.895206\n",
      "MRT: Epoch 1 : Covered 3380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.525803 : Minibatch training accuracy= 0.907406\n",
      "MRT: Epoch 1 : Covered 3400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.658669 : Minibatch training accuracy= 0.895932\n",
      "MRT: Epoch 1 : Covered 3420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.654850 : Minibatch training accuracy= 0.903198\n",
      "MRT: Epoch 1 : Covered 3440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.065576 : Minibatch training accuracy= 0.895583\n",
      "MRT: Epoch 1 : Covered 3460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.340832 : Minibatch training accuracy= 0.891745\n",
      "MRT: Epoch 1 : Covered 3480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.915793 : Minibatch training accuracy= 0.903707\n",
      "MRT: Epoch 1 : Covered 3500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.636274 : Minibatch training accuracy= 0.897967\n",
      "MRT: Epoch 1 : Covered 3520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.494593 : Minibatch training accuracy= 0.906366\n",
      "MRT: Epoch 1 : Covered 3540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.445201 : Minibatch training accuracy= 0.905023\n",
      "MRT: Epoch 1 : Covered 3560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.225227 : Minibatch training accuracy= 0.910632\n",
      "MRT: Epoch 1 : Covered 3580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.456549 : Minibatch training accuracy= 0.862128\n",
      "MRT: Epoch 1 : Covered 3600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.422747 : Minibatch training accuracy= 0.897393\n",
      "MRT: Epoch 1 : Covered 3620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.929955 : Minibatch training accuracy= 0.868143\n",
      "MRT: Epoch 1 : Covered 3640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.645942 : Minibatch training accuracy= 0.901148\n",
      "MRT: Epoch 1 : Covered 3660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.282722 : Minibatch training accuracy= 0.909997\n",
      "MRT: Epoch 1 : Covered 3680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.376437 : Minibatch training accuracy= 0.912677\n",
      "MRT: Epoch 1 : Covered 3700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.850241 : Minibatch training accuracy= 0.916776\n",
      "MRT: Epoch 1 : Covered 3720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.415974 : Minibatch training accuracy= 0.911091\n",
      "MRT: Epoch 1 : Covered 3740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.446762 : Minibatch training accuracy= 0.899854\n",
      "MRT: Epoch 1 : Covered 3760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.200376 : Minibatch training accuracy= 0.881960\n",
      "MRT: Epoch 1 : Covered 3780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.302008 : Minibatch training accuracy= 0.883442\n",
      "MRT: Epoch 1 : Covered 3800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.984216 : Minibatch training accuracy= 0.888297\n",
      "MRT: Epoch 1 : Covered 3820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.444768 : Minibatch training accuracy= 0.906097\n",
      "MRT: Epoch 1 : Covered 3840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.805933 : Minibatch training accuracy= 0.902880\n",
      "MRT: Epoch 1 : Covered 3860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.493129 : Minibatch training accuracy= 0.921852\n",
      "MRT: Epoch 1 : Covered 3880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.532593 : Minibatch training accuracy= 0.889394\n",
      "MRT: Epoch 1 : Covered 3900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.118684 : Minibatch training accuracy= 0.902159\n",
      "MRT: Epoch 1 : Covered 3920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.322685 : Minibatch training accuracy= 0.877806\n",
      "MRT: Epoch 1 : Covered 3940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.236882 : Minibatch training accuracy= 0.910655\n",
      "MRT: Epoch 1 : Covered 3960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.274462 : Minibatch training accuracy= 0.900057\n",
      "MRT: Epoch 1 : Covered 3980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.132898 : Minibatch training accuracy= 0.886063\n",
      "MRT: Epoch 1 : Covered 4000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.030001 : Minibatch training accuracy= 0.892118\n",
      "MRT: Epoch 1 : Covered 4020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.527432 : Minibatch training accuracy= 0.900080\n",
      "MRT: Epoch 1 : Covered 4040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.676156 : Minibatch training accuracy= 0.880856\n",
      "MRT: Epoch 1 : Covered 4060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.502617 : Minibatch training accuracy= 0.877796\n",
      "MRT: Epoch 1 : Covered 4080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.976052 : Minibatch training accuracy= 0.877123\n",
      "MRT: Epoch 1 : Covered 4100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.360614 : Minibatch training accuracy= 0.894770\n",
      "MRT: Epoch 1 : Covered 4120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.467932 : Minibatch training accuracy= 0.879337\n",
      "MRT: Epoch 1 : Covered 4140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.602824 : Minibatch training accuracy= 0.905976\n",
      "MRT: Epoch 1 : Covered 4160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.048858 : Minibatch training accuracy= 0.903508\n",
      "MRT: Epoch 1 : Covered 4180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.465729 : Minibatch training accuracy= 0.900318\n",
      "MRT: Epoch 1 : Covered 4200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.161862 : Minibatch training accuracy= 0.895464\n",
      "MRT: Epoch 1 : Covered 4220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.250935 : Minibatch training accuracy= 0.910093\n",
      "MRT: Epoch 1 : Covered 4240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.261459 : Minibatch training accuracy= 0.891584\n",
      "MRT: Epoch 1 : Covered 4260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.212564 : Minibatch training accuracy= 0.912356\n",
      "MRT: Epoch 1 : Covered 4280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.163898 : Minibatch training accuracy= 0.890375\n",
      "MRT: Epoch 1 : Covered 4300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.147105 : Minibatch training accuracy= 0.888954\n",
      "MRT: Epoch 1 : Covered 4320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.198791 : Minibatch training accuracy= 0.910964\n",
      "MRT: Epoch 1 : Covered 4340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.637921 : Minibatch training accuracy= 0.873972\n",
      "MRT: Epoch 1 : Covered 4360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.205218 : Minibatch training accuracy= 0.880937\n",
      "MRT: Epoch 1 : Covered 4380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.651929 : Minibatch training accuracy= 0.870233\n",
      "MRT: Epoch 1 : Covered 4400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.481477 : Minibatch training accuracy= 0.922091\n",
      "MRT: Epoch 1 : Covered 4420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.023152 : Minibatch training accuracy= 0.898718\n",
      "MRT: Epoch 1 : Covered 4440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.275506 : Minibatch training accuracy= 0.878890\n",
      "MRT: Epoch 1 : Covered 4460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.979323 : Minibatch training accuracy= 0.879877\n",
      "MRT: Epoch 1 : Covered 4480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.687204 : Minibatch training accuracy= 0.888117\n",
      "MRT: Epoch 1 : Covered 4500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.399859 : Minibatch training accuracy= 0.892495\n",
      "MRT: Epoch 1 : Covered 4520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.263981 : Minibatch training accuracy= 0.909218\n",
      "MRT: Epoch 1 : Covered 4540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.197424 : Minibatch training accuracy= 0.905172\n",
      "MRT: Epoch 1 : Covered 4560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.863934 : Minibatch training accuracy= 0.894669\n",
      "MRT: Epoch 1 : Covered 4580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.416996 : Minibatch training accuracy= 0.909837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 4600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.324103 : Minibatch training accuracy= 0.892758\n",
      "MRT: Epoch 1 : Covered 4620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.113213 : Minibatch training accuracy= 0.897056\n",
      "MRT: Epoch 1 : Covered 4640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.278108 : Minibatch training accuracy= 0.874340\n",
      "MRT: Epoch 1 : Covered 4660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.186037 : Minibatch training accuracy= 0.910586\n",
      "MRT: Epoch 1 : Covered 4680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.271154 : Minibatch training accuracy= 0.874293\n",
      "MRT: Epoch 1 : Covered 4700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.314471 : Minibatch training accuracy= 0.901464\n",
      "MRT: Epoch 1 : Covered 4720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.428080 : Minibatch training accuracy= 0.893723\n",
      "MRT: Epoch 1 : Covered 4740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.449124 : Minibatch training accuracy= 0.853466\n",
      "MRT: Epoch 1 : Covered 4760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.143684 : Minibatch training accuracy= 0.884632\n",
      "MRT: Epoch 1 : Covered 4780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.184063 : Minibatch training accuracy= 0.890758\n",
      "MRT: Epoch 1 : Covered 4800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.376746 : Minibatch training accuracy= 0.899364\n",
      "MRT: Epoch 1 : Covered 4820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.117138 : Minibatch training accuracy= 0.926865\n",
      "MRT: Epoch 1 : Covered 4840/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.263522 : Minibatch training accuracy= 0.894424\n",
      "MRT: Epoch 1 : Covered 4860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.029400 : Minibatch training accuracy= 0.855070\n",
      "MRT: Epoch 1 : Covered 4880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.292656 : Minibatch training accuracy= 0.899831\n",
      "MRT: Epoch 1 : Covered 4900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.347022 : Minibatch training accuracy= 0.905326\n",
      "MRT: Epoch 1 : Covered 4920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.626482 : Minibatch training accuracy= 0.890909\n",
      "MRT: Epoch 1 : Covered 4940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.124032 : Minibatch training accuracy= 0.886679\n",
      "MRT: Epoch 1 : Covered 4960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.407909 : Minibatch training accuracy= 0.910710\n",
      "MRT: Epoch 1 : Covered 4980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.073758 : Minibatch training accuracy= 0.913091\n",
      "MRT: Epoch 1 : Covered 5000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.966448 : Minibatch training accuracy= 0.904106\n",
      "MRT: Epoch 1 : Covered 5020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.685014 : Minibatch training accuracy= 0.909120\n",
      "MRT: Epoch 1 : Covered 5040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.152469 : Minibatch training accuracy= 0.903161\n",
      "MRT: Epoch 1 : Covered 5060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.416275 : Minibatch training accuracy= 0.894331\n",
      "MRT: Epoch 1 : Covered 5080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.464631 : Minibatch training accuracy= 0.901507\n",
      "MRT: Epoch 1 : Covered 5100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.035100 : Minibatch training accuracy= 0.901722\n",
      "MRT: Epoch 1 : Covered 5120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.205755 : Minibatch training accuracy= 0.878653\n",
      "MRT: Epoch 1 : Covered 5140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.601822 : Minibatch training accuracy= 0.895310\n",
      "MRT: Epoch 1 : Covered 5160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.446223 : Minibatch training accuracy= 0.918862\n",
      "MRT: Epoch 1 : Covered 5180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.424729 : Minibatch training accuracy= 0.895346\n",
      "MRT: Epoch 1 : Covered 5200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.354501 : Minibatch training accuracy= 0.916623\n",
      "MRT: Epoch 1 : Covered 5220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.095188 : Minibatch training accuracy= 0.903533\n",
      "MRT: Epoch 1 : Covered 5240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.066460 : Minibatch training accuracy= 0.899023\n",
      "MRT: Epoch 1 : Covered 5260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.182815 : Minibatch training accuracy= 0.883316\n",
      "MRT: Epoch 1 : Covered 5280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.758010 : Minibatch training accuracy= 0.905541\n",
      "MRT: Epoch 1 : Covered 5300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.860788 : Minibatch training accuracy= 0.891367\n",
      "MRT: Epoch 1 : Covered 5320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.740828 : Minibatch training accuracy= 0.880161\n",
      "MRT: Epoch 1 : Covered 5340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.258641 : Minibatch training accuracy= 0.897464\n",
      "MRT: Epoch 1 : Covered 5360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.601113 : Minibatch training accuracy= 0.910998\n",
      "MRT: Epoch 1 : Covered 5380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.596272 : Minibatch training accuracy= 0.891443\n",
      "MRT: Epoch 1 : Covered 5400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.313348 : Minibatch training accuracy= 0.908989\n",
      "MRT: Epoch 1 : Covered 5420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.883933 : Minibatch training accuracy= 0.911727\n",
      "MRT: Epoch 1 : Covered 5440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.357715 : Minibatch training accuracy= 0.863531\n",
      "MRT: Epoch 1 : Covered 5460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.638537 : Minibatch training accuracy= 0.912078\n",
      "MRT: Epoch 1 : Covered 5480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.435546 : Minibatch training accuracy= 0.900512\n",
      "MRT: Epoch 1 : Covered 5500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.791050 : Minibatch training accuracy= 0.866345\n",
      "MRT: Epoch 1 : Covered 5520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.174228 : Minibatch training accuracy= 0.923601\n",
      "MRT: Epoch 1 : Covered 5540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.360436 : Minibatch training accuracy= 0.926225\n",
      "MRT: Epoch 1 : Covered 5560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.377195 : Minibatch training accuracy= 0.883123\n",
      "MRT: Epoch 1 : Covered 5580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.137869 : Minibatch training accuracy= 0.908797\n",
      "MRT: Epoch 1 : Covered 5600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.523456 : Minibatch training accuracy= 0.897644\n",
      "MRT: Epoch 1 : Covered 5620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.815223 : Minibatch training accuracy= 0.881805\n",
      "MRT: Epoch 1 : Covered 5640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.475317 : Minibatch training accuracy= 0.890165\n",
      "MRT: Epoch 1 : Covered 5660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.051697 : Minibatch training accuracy= 0.907000\n",
      "MRT: Epoch 1 : Covered 5680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.234477 : Minibatch training accuracy= 0.910306\n",
      "MRT: Epoch 1 : Covered 5700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.256559 : Minibatch training accuracy= 0.925579\n",
      "MRT: Epoch 1 : Covered 5720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.711597 : Minibatch training accuracy= 0.896480\n",
      "MRT: Epoch 1 : Covered 5740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.417053 : Minibatch training accuracy= 0.886316\n",
      "MRT: Epoch 1 : Covered 5760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.433526 : Minibatch training accuracy= 0.913932\n",
      "MRT: Epoch 1 : Covered 5780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.226773 : Minibatch training accuracy= 0.881422\n",
      "MRT: Epoch 1 : Covered 5800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.876406 : Minibatch training accuracy= 0.916433\n",
      "MRT: Epoch 1 : Covered 5820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.352379 : Minibatch training accuracy= 0.902523\n",
      "MRT: Epoch 1 : Covered 5840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.414245 : Minibatch training accuracy= 0.853713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 5860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.974160 : Minibatch training accuracy= 0.903431\n",
      "MRT: Epoch 1 : Covered 5880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.800306 : Minibatch training accuracy= 0.863124\n",
      "MRT: Epoch 1 : Covered 5900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.667919 : Minibatch training accuracy= 0.896233\n",
      "MRT: Epoch 1 : Covered 5920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.062467 : Minibatch training accuracy= 0.905894\n",
      "MRT: Epoch 1 : Covered 5940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.253256 : Minibatch training accuracy= 0.882283\n",
      "MRT: Epoch 1 : Covered 5960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.453531 : Minibatch training accuracy= 0.883942\n",
      "MRT: Epoch 1 : Covered 5980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.392030 : Minibatch training accuracy= 0.891870\n",
      "MRT: Epoch 1 : Covered 6000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.416580 : Minibatch training accuracy= 0.883314\n",
      "MRT: Epoch 1 : Covered 6020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.650936 : Minibatch training accuracy= 0.883514\n",
      "MRT: Epoch 1 : Covered 6040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.232028 : Minibatch training accuracy= 0.891523\n",
      "MRT: Epoch 1 : Covered 6060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.090500 : Minibatch training accuracy= 0.889836\n",
      "MRT: Epoch 1 : Covered 6080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.252553 : Minibatch training accuracy= 0.879202\n",
      "MRT: Epoch 1 : Covered 6100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.646141 : Minibatch training accuracy= 0.910254\n",
      "MRT: Epoch 1 : Covered 6120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.097008 : Minibatch training accuracy= 0.869778\n",
      "MRT: Epoch 1 : Covered 6140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.649094 : Minibatch training accuracy= 0.900877\n",
      "MRT: Epoch 1 : Covered 6160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.301430 : Minibatch training accuracy= 0.882264\n",
      "MRT: Epoch 1 : Covered 6180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.747547 : Minibatch training accuracy= 0.932922\n",
      "MRT: Epoch 1 : Covered 6200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.074787 : Minibatch training accuracy= 0.906001\n",
      "MRT: Epoch 1 : Covered 6220/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.002875 : Minibatch training accuracy= 0.901509\n",
      "MRT: Epoch 1 : Covered 6240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.383038 : Minibatch training accuracy= 0.911402\n",
      "MRT: Epoch 1 : Covered 6260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.425735 : Minibatch training accuracy= 0.900811\n",
      "MRT: Epoch 1 : Covered 6280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.283765 : Minibatch training accuracy= 0.889886\n",
      "MRT: Epoch 1 : Covered 6300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.509896 : Minibatch training accuracy= 0.887679\n",
      "MRT: Epoch 1 : Covered 6320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.044885 : Minibatch training accuracy= 0.884176\n",
      "MRT: Epoch 1 : Covered 6340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.658803 : Minibatch training accuracy= 0.910629\n",
      "MRT: Epoch 1 : Covered 6360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.985184 : Minibatch training accuracy= 0.856364\n",
      "MRT: Epoch 1 : Covered 6380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.317704 : Minibatch training accuracy= 0.923010\n",
      "MRT: Epoch 1 : Covered 6400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.882293 : Minibatch training accuracy= 0.843978\n",
      "MRT: Epoch 1 : Covered 6420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.545405 : Minibatch training accuracy= 0.914870\n",
      "MRT: Epoch 1 : Covered 6440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.304998 : Minibatch training accuracy= 0.877532\n",
      "MRT: Epoch 1 : Covered 6460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.577646 : Minibatch training accuracy= 0.897378\n",
      "MRT: Epoch 1 : Covered 6480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.652257 : Minibatch training accuracy= 0.893018\n",
      "MRT: Epoch 1 : Covered 6500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.279887 : Minibatch training accuracy= 0.899259\n",
      "MRT: Epoch 1 : Covered 6520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.877862 : Minibatch training accuracy= 0.885236\n",
      "MRT: Epoch 1 : Covered 6540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.261229 : Minibatch training accuracy= 0.896676\n",
      "MRT: Epoch 1 : Covered 6560/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.911140 : Minibatch training accuracy= 0.878622\n",
      "MRT: Epoch 1 : Covered 6580/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.888478 : Minibatch training accuracy= 0.902664\n",
      "MRT: Epoch 1 : Covered 6600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.434109 : Minibatch training accuracy= 0.888638\n",
      "MRT: Epoch 1 : Covered 6620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.297080 : Minibatch training accuracy= 0.909383\n",
      "MRT: Epoch 1 : Covered 6640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.309081 : Minibatch training accuracy= 0.905130\n",
      "MRT: Epoch 1 : Covered 6660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.780321 : Minibatch training accuracy= 0.872149\n",
      "MRT: Epoch 1 : Covered 6680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.201117 : Minibatch training accuracy= 0.919283\n",
      "MRT: Epoch 1 : Covered 6700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.855151 : Minibatch training accuracy= 0.877364\n",
      "MRT: Epoch 1 : Covered 6720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.118198 : Minibatch training accuracy= 0.885718\n",
      "MRT: Epoch 1 : Covered 6740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.614596 : Minibatch training accuracy= 0.908821\n",
      "MRT: Epoch 1 : Covered 6760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.190066 : Minibatch training accuracy= 0.881245\n",
      "MRT: Epoch 1 : Covered 6780/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.970465 : Minibatch training accuracy= 0.918892\n",
      "MRT: Epoch 1 : Covered 6800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.477127 : Minibatch training accuracy= 0.899262\n",
      "MRT: Epoch 1 : Covered 6820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.659106 : Minibatch training accuracy= 0.891037\n",
      "MRT: Epoch 1 : Covered 6840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.206286 : Minibatch training accuracy= 0.883744\n",
      "MRT: Epoch 1 : Covered 6860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.824576 : Minibatch training accuracy= 0.914104\n",
      "MRT: Epoch 1 : Covered 6880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.200927 : Minibatch training accuracy= 0.888613\n",
      "MRT: Epoch 1 : Covered 6900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.588298 : Minibatch training accuracy= 0.919009\n",
      "MRT: Epoch 1 : Covered 6920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.375602 : Minibatch training accuracy= 0.921616\n",
      "MRT: Epoch 1 : Covered 6940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.139957 : Minibatch training accuracy= 0.900177\n",
      "MRT: Epoch 1 : Covered 6960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.606111 : Minibatch training accuracy= 0.872395\n",
      "MRT: Epoch 1 : Covered 6980/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.841862 : Minibatch training accuracy= 0.886470\n",
      "MRT: Epoch 1 : Covered 7000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.447942 : Minibatch training accuracy= 0.887542\n",
      "MRT: Epoch 1 : Covered 7020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.143054 : Minibatch training accuracy= 0.895318\n",
      "MRT: Epoch 1 : Covered 7040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.153980 : Minibatch training accuracy= 0.901368\n",
      "MRT: Epoch 1 : Covered 7060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.094738 : Minibatch training accuracy= 0.888370\n",
      "MRT: Epoch 1 : Covered 7080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.186238 : Minibatch training accuracy= 0.919412\n",
      "MRT: Epoch 1 : Covered 7100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.448592 : Minibatch training accuracy= 0.897203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 7120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.104792 : Minibatch training accuracy= 0.871282\n",
      "MRT: Epoch 1 : Covered 7140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.167991 : Minibatch training accuracy= 0.879480\n",
      "MRT: Epoch 1 : Covered 7160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.623658 : Minibatch training accuracy= 0.882176\n",
      "MRT: Epoch 1 : Covered 7180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.349042 : Minibatch training accuracy= 0.886700\n",
      "MRT: Epoch 1 : Covered 7200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.218615 : Minibatch training accuracy= 0.879721\n",
      "MRT: Epoch 1 : Covered 7220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.041978 : Minibatch training accuracy= 0.913482\n",
      "MRT: Epoch 1 : Covered 7240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.266428 : Minibatch training accuracy= 0.889695\n",
      "MRT: Epoch 1 : Covered 7260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.120045 : Minibatch training accuracy= 0.878204\n",
      "MRT: Epoch 1 : Covered 7280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.510998 : Minibatch training accuracy= 0.906631\n",
      "MRT: Epoch 1 : Covered 7300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.094041 : Minibatch training accuracy= 0.907763\n",
      "MRT: Epoch 1 : Covered 7320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.758379 : Minibatch training accuracy= 0.919144\n",
      "MRT: Epoch 1 : Covered 7340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.249464 : Minibatch training accuracy= 0.903881\n",
      "MRT: Epoch 1 : Covered 7360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.622208 : Minibatch training accuracy= 0.889290\n",
      "MRT: Epoch 1 : Covered 7380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.610823 : Minibatch training accuracy= 0.873848\n",
      "MRT: Epoch 1 : Covered 7400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.343395 : Minibatch training accuracy= 0.916402\n",
      "MRT: Epoch 1 : Covered 7420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.061460 : Minibatch training accuracy= 0.902656\n",
      "MRT: Epoch 1 : Covered 7440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.271189 : Minibatch training accuracy= 0.893633\n",
      "MRT: Epoch 1 : Covered 7460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.323645 : Minibatch training accuracy= 0.902001\n",
      "MRT: Epoch 1 : Covered 7480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.438058 : Minibatch training accuracy= 0.888028\n",
      "MRT: Epoch 1 : Covered 7500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.999495 : Minibatch training accuracy= 0.878244\n",
      "MRT: Epoch 1 : Covered 7520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.185669 : Minibatch training accuracy= 0.867836\n",
      "MRT: Epoch 1 : Covered 7540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.662535 : Minibatch training accuracy= 0.885017\n",
      "MRT: Epoch 1 : Covered 7560/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.007042 : Minibatch training accuracy= 0.879449\n",
      "MRT: Epoch 1 : Covered 7580/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.945961 : Minibatch training accuracy= 0.858242\n",
      "MRT: Epoch 1 : Covered 7600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.150490 : Minibatch training accuracy= 0.890171\n",
      "MRT: Epoch 1 : Covered 7620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.544896 : Minibatch training accuracy= 0.912906\n",
      "MRT: Epoch 1 : Covered 7640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.587261 : Minibatch training accuracy= 0.882387\n",
      "MRT: Epoch 1 : Covered 7660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.234361 : Minibatch training accuracy= 0.906486\n",
      "MRT: Epoch 1 : Covered 7680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.177820 : Minibatch training accuracy= 0.876712\n",
      "MRT: Epoch 1 : Covered 7700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.965941 : Minibatch training accuracy= 0.911358\n",
      "MRT: Epoch 1 : Covered 7720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.407753 : Minibatch training accuracy= 0.906629\n",
      "MRT: Epoch 1 : Covered 7740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.487786 : Minibatch training accuracy= 0.899374\n",
      "MRT: Epoch 1 : Covered 7760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.007136 : Minibatch training accuracy= 0.866370\n",
      "MRT: Epoch 1 : Covered 7780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.388179 : Minibatch training accuracy= 0.873479\n",
      "MRT: Epoch 1 : Covered 7800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.985331 : Minibatch training accuracy= 0.916282\n",
      "MRT: Epoch 1 : Covered 7820/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.000642 : Minibatch training accuracy= 0.912922\n",
      "MRT: Epoch 1 : Covered 7840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.219480 : Minibatch training accuracy= 0.851536\n",
      "MRT: Epoch 1 : Covered 7860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.766447 : Minibatch training accuracy= 0.898932\n",
      "MRT: Epoch 1 : Covered 7880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.330531 : Minibatch training accuracy= 0.904973\n",
      "MRT: Epoch 1 : Covered 7900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.923922 : Minibatch training accuracy= 0.870365\n",
      "MRT: Epoch 1 : Covered 7920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.838972 : Minibatch training accuracy= 0.890920\n",
      "MRT: Epoch 1 : Covered 7940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.262895 : Minibatch training accuracy= 0.885099\n",
      "MRT: Epoch 1 : Covered 7960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.095879 : Minibatch training accuracy= 0.885263\n",
      "MRT: Epoch 1 : Covered 7980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.346251 : Minibatch training accuracy= 0.922633\n",
      "MRT: Epoch 1 : Covered 8000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.209673 : Minibatch training accuracy= 0.878136\n",
      "MRT: Epoch 1 : Covered 8020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.083040 : Minibatch training accuracy= 0.877104\n",
      "MRT: Epoch 1 : Covered 8040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.852745 : Minibatch training accuracy= 0.860824\n",
      "MRT: Epoch 1 : Covered 8060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.895789 : Minibatch training accuracy= 0.898704\n",
      "MRT: Epoch 1 : Covered 8080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.547882 : Minibatch training accuracy= 0.891678\n",
      "MRT: Epoch 1 : Covered 8100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.445681 : Minibatch training accuracy= 0.883198\n",
      "MRT: Epoch 1 : Covered 8120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.983967 : Minibatch training accuracy= 0.881253\n",
      "MRT: Epoch 1 : Covered 8140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.251879 : Minibatch training accuracy= 0.911884\n",
      "MRT: Epoch 1 : Covered 8160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.525510 : Minibatch training accuracy= 0.906852\n",
      "MRT: Epoch 1 : Covered 8180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.257493 : Minibatch training accuracy= 0.900279\n",
      "MRT: Epoch 1 : Covered 8200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.244045 : Minibatch training accuracy= 0.906972\n",
      "MRT: Epoch 1 : Covered 8220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.693404 : Minibatch training accuracy= 0.901537\n",
      "MRT: Epoch 1 : Covered 8240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.436118 : Minibatch training accuracy= 0.867583\n",
      "MRT: Epoch 1 : Covered 8260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.558098 : Minibatch training accuracy= 0.886964\n",
      "MRT: Epoch 1 : Covered 8280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.953914 : Minibatch training accuracy= 0.870550\n",
      "MRT: Epoch 1 : Covered 8300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.386763 : Minibatch training accuracy= 0.878928\n",
      "MRT: Epoch 1 : Covered 8320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.214462 : Minibatch training accuracy= 0.894804\n",
      "MRT: Epoch 1 : Covered 8340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.079345 : Minibatch training accuracy= 0.893020\n",
      "MRT: Epoch 1 : Covered 8360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.076135 : Minibatch training accuracy= 0.870367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 8380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.034508 : Minibatch training accuracy= 0.909261\n",
      "MRT: Epoch 1 : Covered 8400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.257290 : Minibatch training accuracy= 0.864768\n",
      "MRT: Epoch 1 : Covered 8420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.880491 : Minibatch training accuracy= 0.912165\n",
      "MRT: Epoch 1 : Covered 8440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.727395 : Minibatch training accuracy= 0.910078\n",
      "MRT: Epoch 1 : Covered 8460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.291090 : Minibatch training accuracy= 0.880257\n",
      "MRT: Epoch 1 : Covered 8480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.809936 : Minibatch training accuracy= 0.909613\n",
      "MRT: Epoch 1 : Covered 8500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.958190 : Minibatch training accuracy= 0.840926\n",
      "MRT: Epoch 1 : Covered 8520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.999700 : Minibatch training accuracy= 0.887008\n",
      "MRT: Epoch 1 : Covered 8540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.354651 : Minibatch training accuracy= 0.900308\n",
      "MRT: Epoch 1 : Covered 8560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.453304 : Minibatch training accuracy= 0.872418\n",
      "MRT: Epoch 1 : Covered 8580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.448227 : Minibatch training accuracy= 0.899836\n",
      "MRT: Epoch 1 : Covered 8600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.106914 : Minibatch training accuracy= 0.900592\n",
      "MRT: Epoch 1 : Covered 8620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.193110 : Minibatch training accuracy= 0.863085\n",
      "MRT: Epoch 1 : Covered 8640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.309876 : Minibatch training accuracy= 0.900334\n",
      "MRT: Epoch 1 : Covered 8660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.021571 : Minibatch training accuracy= 0.889441\n",
      "MRT: Epoch 1 : Covered 8680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.401911 : Minibatch training accuracy= 0.903251\n",
      "MRT: Epoch 1 : Covered 8700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.124895 : Minibatch training accuracy= 0.890356\n",
      "MRT: Epoch 1 : Covered 8720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.305768 : Minibatch training accuracy= 0.896284\n",
      "MRT: Epoch 1 : Covered 8740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.638562 : Minibatch training accuracy= 0.881920\n",
      "MRT: Epoch 1 : Covered 8760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.533763 : Minibatch training accuracy= 0.922836\n",
      "MRT: Epoch 1 : Covered 8780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.455221 : Minibatch training accuracy= 0.923509\n",
      "MRT: Epoch 1 : Covered 8800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.039967 : Minibatch training accuracy= 0.882997\n",
      "MRT: Epoch 1 : Covered 8820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.331322 : Minibatch training accuracy= 0.888525\n",
      "MRT: Epoch 1 : Covered 8840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.739760 : Minibatch training accuracy= 0.906534\n",
      "MRT: Epoch 1 : Covered 8860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.449183 : Minibatch training accuracy= 0.868769\n",
      "MRT: Epoch 1 : Covered 8880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.141270 : Minibatch training accuracy= 0.893349\n",
      "MRT: Epoch 1 : Covered 8900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.989386 : Minibatch training accuracy= 0.871266\n",
      "MRT: Epoch 1 : Covered 8920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.001748 : Minibatch training accuracy= 0.861788\n",
      "MRT: Epoch 1 : Covered 8940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.791732 : Minibatch training accuracy= 0.894605\n",
      "MRT: Epoch 1 : Covered 8960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.721151 : Minibatch training accuracy= 0.886344\n",
      "MRT: Epoch 1 : Covered 8980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.352384 : Minibatch training accuracy= 0.855589\n",
      "MRT: Epoch 1 : Covered 9000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.207153 : Minibatch training accuracy= 0.881190\n",
      "MRT: Epoch 1 : Covered 9020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.408597 : Minibatch training accuracy= 0.896314\n",
      "MRT: Epoch 1 : Covered 9040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.148870 : Minibatch training accuracy= 0.899439\n",
      "MRT: Epoch 1 : Covered 9060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.262013 : Minibatch training accuracy= 0.890806\n",
      "MRT: Epoch 1 : Covered 9080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.275438 : Minibatch training accuracy= 0.908467\n",
      "MRT: Epoch 1 : Covered 9100/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.698301 : Minibatch training accuracy= 0.888715\n",
      "MRT: Epoch 1 : Covered 9120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.953140 : Minibatch training accuracy= 0.910784\n",
      "MRT: Epoch 1 : Covered 9140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.123203 : Minibatch training accuracy= 0.889490\n",
      "MRT: Epoch 1 : Covered 9160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.594586 : Minibatch training accuracy= 0.884107\n",
      "MRT: Epoch 1 : Covered 9180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.540230 : Minibatch training accuracy= 0.885333\n",
      "MRT: Epoch 1 : Covered 9200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.029480 : Minibatch training accuracy= 0.870306\n",
      "MRT: Epoch 1 : Covered 9220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.015266 : Minibatch training accuracy= 0.882379\n",
      "MRT: Epoch 1 : Covered 9240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.414913 : Minibatch training accuracy= 0.888110\n",
      "MRT: Epoch 1 : Covered 9260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.068346 : Minibatch training accuracy= 0.839355\n",
      "MRT: Epoch 1 : Covered 9280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.269040 : Minibatch training accuracy= 0.907179\n",
      "MRT: Epoch 1 : Covered 9300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.249979 : Minibatch training accuracy= 0.885360\n",
      "MRT: Epoch 1 : Covered 9320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.293754 : Minibatch training accuracy= 0.897014\n",
      "MRT: Epoch 1 : Covered 9340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.314923 : Minibatch training accuracy= 0.897622\n",
      "MRT: Epoch 1 : Covered 9360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.445795 : Minibatch training accuracy= 0.884523\n",
      "MRT: Epoch 1 : Covered 9380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.183976 : Minibatch training accuracy= 0.879020\n",
      "MRT: Epoch 1 : Covered 9400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.772647 : Minibatch training accuracy= 0.907748\n",
      "MRT: Epoch 1 : Covered 9420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.390315 : Minibatch training accuracy= 0.899540\n",
      "MRT: Epoch 1 : Covered 9440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.502789 : Minibatch training accuracy= 0.892153\n",
      "MRT: Epoch 1 : Covered 9460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.144290 : Minibatch training accuracy= 0.862470\n",
      "MRT: Epoch 1 : Covered 9480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.114641 : Minibatch training accuracy= 0.883798\n",
      "MRT: Epoch 1 : Covered 9500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.356410 : Minibatch training accuracy= 0.906109\n",
      "MRT: Epoch 1 : Covered 9520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.317265 : Minibatch training accuracy= 0.886965\n",
      "MRT: Epoch 1 : Covered 9540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.566977 : Minibatch training accuracy= 0.902073\n",
      "MRT: Epoch 1 : Covered 9560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.311936 : Minibatch training accuracy= 0.895694\n",
      "MRT: Epoch 1 : Covered 9580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.799702 : Minibatch training accuracy= 0.887905\n",
      "MRT: Epoch 1 : Covered 9600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.285016 : Minibatch training accuracy= 0.889633\n",
      "MRT: Epoch 1 : Covered 9620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.912311 : Minibatch training accuracy= 0.852086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 9640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.373516 : Minibatch training accuracy= 0.908536\n",
      "MRT: Epoch 1 : Covered 9660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.162474 : Minibatch training accuracy= 0.897852\n",
      "MRT: Epoch 1 : Covered 9680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.229955 : Minibatch training accuracy= 0.904727\n",
      "MRT: Epoch 1 : Covered 9700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.010909 : Minibatch training accuracy= 0.882788\n",
      "MRT: Epoch 1 : Covered 9720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.563011 : Minibatch training accuracy= 0.883545\n",
      "MRT: Epoch 1 : Covered 9740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.978799 : Minibatch training accuracy= 0.874515\n",
      "MRT: Epoch 1 : Covered 9760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.243945 : Minibatch training accuracy= 0.899888\n",
      "MRT: Epoch 1 : Covered 9780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.697173 : Minibatch training accuracy= 0.903014\n",
      "MRT: Epoch 1 : Covered 9800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.536811 : Minibatch training accuracy= 0.904038\n",
      "MRT: Epoch 1 : Covered 9820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.320836 : Minibatch training accuracy= 0.904413\n",
      "MRT: Epoch 1 : Covered 9840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.827825 : Minibatch training accuracy= 0.834375\n",
      "MRT: Epoch 1 : Covered 9860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.077967 : Minibatch training accuracy= 0.887931\n",
      "MRT: Epoch 1 : Covered 9880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.893199 : Minibatch training accuracy= 0.895403\n",
      "MRT: Epoch 1 : Covered 9900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.228137 : Minibatch training accuracy= 0.902011\n",
      "MRT: Epoch 1 : Covered 9920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.407451 : Minibatch training accuracy= 0.887991\n",
      "MRT: Epoch 1 : Covered 9940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.896874 : Minibatch training accuracy= 0.873976\n",
      "MRT: Epoch 1 : Covered 9960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.976951 : Minibatch training accuracy= 0.879561\n",
      "MRT: Epoch 1 : Covered 9980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.848402 : Minibatch training accuracy= 0.876538\n",
      "MRT: Epoch 1 : Covered 10000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.116182 : Minibatch training accuracy= 0.873646\n",
      "MRT: Epoch 1 : Covered 10020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.123107 : Minibatch training accuracy= 0.894763\n",
      "MRT: Epoch 1 : Covered 10040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.137783 : Minibatch training accuracy= 0.898569\n",
      "MRT: Epoch 1 : Covered 10060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.651034 : Minibatch training accuracy= 0.825197\n",
      "MRT: Epoch 1 : Covered 10080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.405640 : Minibatch training accuracy= 0.900578\n",
      "MRT: Epoch 1 : Covered 10100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.442745 : Minibatch training accuracy= 0.904415\n",
      "MRT: Epoch 1 : Covered 10120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.358691 : Minibatch training accuracy= 0.906310\n",
      "MRT: Epoch 1 : Covered 10140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.516087 : Minibatch training accuracy= 0.922464\n",
      "MRT: Epoch 1 : Covered 10160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.126738 : Minibatch training accuracy= 0.891610\n",
      "MRT: Epoch 1 : Covered 10180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.190605 : Minibatch training accuracy= 0.903388\n",
      "MRT: Epoch 1 : Covered 10200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.073073 : Minibatch training accuracy= 0.900948\n",
      "MRT: Epoch 1 : Covered 10220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.366243 : Minibatch training accuracy= 0.890349\n",
      "MRT: Epoch 1 : Covered 10240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.364556 : Minibatch training accuracy= 0.868970\n",
      "MRT: Epoch 1 : Covered 10260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.740312 : Minibatch training accuracy= 0.912963\n",
      "MRT: Epoch 1 : Covered 10280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.213860 : Minibatch training accuracy= 0.904144\n",
      "MRT: Epoch 1 : Covered 10300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.244964 : Minibatch training accuracy= 0.870262\n",
      "MRT: Epoch 1 : Covered 10320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.022843 : Minibatch training accuracy= 0.881200\n",
      "MRT: Epoch 1 : Covered 10340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.017438 : Minibatch training accuracy= 0.876895\n",
      "MRT: Epoch 1 : Covered 10360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.087644 : Minibatch training accuracy= 0.850395\n",
      "MRT: Epoch 1 : Covered 10380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.842181 : Minibatch training accuracy= 0.873951\n",
      "MRT: Epoch 1 : Covered 10400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.651620 : Minibatch training accuracy= 0.898254\n",
      "MRT: Epoch 1 : Covered 10420/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.761040 : Minibatch training accuracy= 0.915060\n",
      "MRT: Epoch 1 : Covered 10440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.771492 : Minibatch training accuracy= 0.855129\n",
      "MRT: Epoch 1 : Covered 10460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.173154 : Minibatch training accuracy= 0.906934\n",
      "MRT: Epoch 1 : Covered 10480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.387521 : Minibatch training accuracy= 0.878743\n",
      "MRT: Epoch 1 : Covered 10500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.139568 : Minibatch training accuracy= 0.896801\n",
      "MRT: Epoch 1 : Covered 10520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.927565 : Minibatch training accuracy= 0.871498\n",
      "MRT: Epoch 1 : Covered 10540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.641408 : Minibatch training accuracy= 0.890060\n",
      "MRT: Epoch 1 : Covered 10560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.454339 : Minibatch training accuracy= 0.885340\n",
      "MRT: Epoch 1 : Covered 10580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.270063 : Minibatch training accuracy= 0.857681\n",
      "MRT: Epoch 1 : Covered 10600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.472764 : Minibatch training accuracy= 0.913775\n",
      "MRT: Epoch 1 : Covered 10620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.691492 : Minibatch training accuracy= 0.914151\n",
      "MRT: Epoch 1 : Covered 10640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.871379 : Minibatch training accuracy= 0.908464\n",
      "MRT: Epoch 1 : Covered 10660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.094706 : Minibatch training accuracy= 0.855153\n",
      "MRT: Epoch 1 : Covered 10680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.142166 : Minibatch training accuracy= 0.907810\n",
      "MRT: Epoch 1 : Covered 10700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.138879 : Minibatch training accuracy= 0.898580\n",
      "MRT: Epoch 1 : Covered 10720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.427265 : Minibatch training accuracy= 0.908476\n",
      "MRT: Epoch 1 : Covered 10740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.870224 : Minibatch training accuracy= 0.891828\n",
      "MRT: Epoch 1 : Covered 10760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.319714 : Minibatch training accuracy= 0.894823\n",
      "MRT: Epoch 1 : Covered 10780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.196862 : Minibatch training accuracy= 0.893927\n",
      "MRT: Epoch 1 : Covered 10800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.381088 : Minibatch training accuracy= 0.901858\n",
      "MRT: Epoch 1 : Covered 10820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.111541 : Minibatch training accuracy= 0.887708\n",
      "MRT: Epoch 1 : Covered 10840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.409436 : Minibatch training accuracy= 0.898017\n",
      "MRT: Epoch 1 : Covered 10860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.104531 : Minibatch training accuracy= 0.902261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 10880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.979105 : Minibatch training accuracy= 0.890573\n",
      "MRT: Epoch 1 : Covered 10900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.707122 : Minibatch training accuracy= 0.904251\n",
      "MRT: Epoch 1 : Covered 10920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.165843 : Minibatch training accuracy= 0.886522\n",
      "MRT: Epoch 1 : Covered 10940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.951890 : Minibatch training accuracy= 0.885131\n",
      "MRT: Epoch 1 : Covered 10960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.988019 : Minibatch training accuracy= 0.890758\n",
      "MRT: Epoch 1 : Covered 10980/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.912650 : Minibatch training accuracy= 0.889699\n",
      "MRT: Epoch 1 : Covered 11000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.579873 : Minibatch training accuracy= 0.884680\n",
      "MRT: Epoch 1 : Covered 11020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.984608 : Minibatch training accuracy= 0.886799\n",
      "MRT: Epoch 1 : Covered 11040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.584977 : Minibatch training accuracy= 0.868603\n",
      "MRT: Epoch 1 : Covered 11060/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.671341 : Minibatch training accuracy= 0.864556\n",
      "MRT: Epoch 1 : Covered 11080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.929502 : Minibatch training accuracy= 0.867768\n",
      "MRT: Epoch 1 : Covered 11100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.312108 : Minibatch training accuracy= 0.888587\n",
      "MRT: Epoch 1 : Covered 11120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.802758 : Minibatch training accuracy= 0.904562\n",
      "MRT: Epoch 1 : Covered 11140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.990440 : Minibatch training accuracy= 0.888133\n",
      "MRT: Epoch 1 : Covered 11160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.144948 : Minibatch training accuracy= 0.893340\n",
      "MRT: Epoch 1 : Covered 11180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.725329 : Minibatch training accuracy= 0.893628\n",
      "MRT: Epoch 1 : Covered 11200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.539918 : Minibatch training accuracy= 0.897543\n",
      "MRT: Epoch 1 : Covered 11220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.073658 : Minibatch training accuracy= 0.891325\n",
      "MRT: Epoch 1 : Covered 11240/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.652756 : Minibatch training accuracy= 0.881247\n",
      "MRT: Epoch 1 : Covered 11260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.111977 : Minibatch training accuracy= 0.903941\n",
      "MRT: Epoch 1 : Covered 11280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.028627 : Minibatch training accuracy= 0.888100\n",
      "MRT: Epoch 1 : Covered 11300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.901926 : Minibatch training accuracy= 0.884000\n",
      "MRT: Epoch 1 : Covered 11320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.873546 : Minibatch training accuracy= 0.881047\n",
      "MRT: Epoch 1 : Covered 11340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.825488 : Minibatch training accuracy= 0.906541\n",
      "MRT: Epoch 1 : Covered 11360/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.597367 : Minibatch training accuracy= 0.872971\n",
      "MRT: Epoch 1 : Covered 11380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.824281 : Minibatch training accuracy= 0.887772\n",
      "MRT: Epoch 1 : Covered 11400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.953259 : Minibatch training accuracy= 0.887942\n",
      "MRT: Epoch 1 : Covered 11420/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.666629 : Minibatch training accuracy= 0.869470\n",
      "MRT: Epoch 1 : Covered 11440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.274844 : Minibatch training accuracy= 0.858733\n",
      "MRT: Epoch 1 : Covered 11460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.325588 : Minibatch training accuracy= 0.932268\n",
      "MRT: Epoch 1 : Covered 11480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.439966 : Minibatch training accuracy= 0.919978\n",
      "MRT: Epoch 1 : Covered 11500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.550395 : Minibatch training accuracy= 0.875365\n",
      "MRT: Epoch 1 : Covered 11520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.215728 : Minibatch training accuracy= 0.918121\n",
      "MRT: Epoch 1 : Covered 11540/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.710070 : Minibatch training accuracy= 0.864429\n",
      "MRT: Epoch 1 : Covered 11560/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.829001 : Minibatch training accuracy= 0.892377\n",
      "MRT: Epoch 1 : Covered 11580/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.992930 : Minibatch training accuracy= 0.906978\n",
      "MRT: Epoch 1 : Covered 11600/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.739228 : Minibatch training accuracy= 0.876712\n",
      "MRT: Epoch 1 : Covered 11620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.114272 : Minibatch training accuracy= 0.899231\n",
      "MRT: Epoch 1 : Covered 11640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.573176 : Minibatch training accuracy= 0.853870\n",
      "MRT: Epoch 1 : Covered 11660/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.762616 : Minibatch training accuracy= 0.938435\n",
      "MRT: Epoch 1 : Covered 11680/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.044229 : Minibatch training accuracy= 0.940980\n",
      "MRT: Epoch 1 : Covered 11700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.555141 : Minibatch training accuracy= 0.916786\n",
      "MRT: Epoch 1 : Covered 11720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.238356 : Minibatch training accuracy= 0.884215\n",
      "MRT: Epoch 1 : Covered 11740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.815297 : Minibatch training accuracy= 0.895438\n",
      "MRT: Epoch 1 : Covered 11760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.165126 : Minibatch training accuracy= 0.896607\n",
      "MRT: Epoch 1 : Covered 11780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.378991 : Minibatch training accuracy= 0.892123\n",
      "MRT: Epoch 1 : Covered 11800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.393914 : Minibatch training accuracy= 0.854418\n",
      "MRT: Epoch 1 : Covered 11820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.234679 : Minibatch training accuracy= 0.872126\n",
      "MRT: Epoch 1 : Covered 11840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.177576 : Minibatch training accuracy= 0.881113\n",
      "MRT: Epoch 1 : Covered 11860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.431587 : Minibatch training accuracy= 0.890342\n",
      "MRT: Epoch 1 : Covered 11880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.236764 : Minibatch training accuracy= 0.896105\n",
      "MRT: Epoch 1 : Covered 11900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.113710 : Minibatch training accuracy= 0.893833\n",
      "MRT: Epoch 1 : Covered 11920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.018929 : Minibatch training accuracy= 0.905547\n",
      "MRT: Epoch 1 : Covered 11940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.159793 : Minibatch training accuracy= 0.868092\n",
      "MRT: Epoch 1 : Covered 11960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.768824 : Minibatch training accuracy= 0.887774\n",
      "MRT: Epoch 1 : Covered 11980/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.647643 : Minibatch training accuracy= 0.875047\n",
      "MRT: Epoch 1 : Covered 12000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.547311 : Minibatch training accuracy= 0.898859\n",
      "MRT: Epoch 1 : Covered 12020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.992847 : Minibatch training accuracy= 0.883965\n",
      "MRT: Epoch 1 : Covered 12040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.843701 : Minibatch training accuracy= 0.886464\n",
      "MRT: Epoch 1 : Covered 12060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.464399 : Minibatch training accuracy= 0.920742\n",
      "MRT: Epoch 1 : Covered 12080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.750263 : Minibatch training accuracy= 0.897554\n",
      "MRT: Epoch 1 : Covered 12100/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.597368 : Minibatch training accuracy= 0.890747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 12120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.670260 : Minibatch training accuracy= 0.834372\n",
      "MRT: Epoch 1 : Covered 12140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.896524 : Minibatch training accuracy= 0.887451\n",
      "MRT: Epoch 1 : Covered 12160/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.940167 : Minibatch training accuracy= 0.869881\n",
      "MRT: Epoch 1 : Covered 12180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.927538 : Minibatch training accuracy= 0.880654\n",
      "MRT: Epoch 1 : Covered 12200/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.820554 : Minibatch training accuracy= 0.864413\n",
      "MRT: Epoch 1 : Covered 12220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.935176 : Minibatch training accuracy= 0.857679\n",
      "MRT: Epoch 1 : Covered 12240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.258820 : Minibatch training accuracy= 0.862531\n",
      "MRT: Epoch 1 : Covered 12260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.822423 : Minibatch training accuracy= 0.882791\n",
      "MRT: Epoch 1 : Covered 12280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.910988 : Minibatch training accuracy= 0.873506\n",
      "MRT: Epoch 1 : Covered 12300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.017092 : Minibatch training accuracy= 0.892195\n",
      "MRT: Epoch 1 : Covered 12320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.836393 : Minibatch training accuracy= 0.844107\n",
      "MRT: Epoch 1 : Covered 12340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.982663 : Minibatch training accuracy= 0.856218\n",
      "MRT: Epoch 1 : Covered 12360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.117147 : Minibatch training accuracy= 0.908280\n",
      "MRT: Epoch 1 : Covered 12380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.839357 : Minibatch training accuracy= 0.870876\n",
      "MRT: Epoch 1 : Covered 12400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.029588 : Minibatch training accuracy= 0.905626\n",
      "MRT: Epoch 1 : Covered 12420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.320052 : Minibatch training accuracy= 0.875016\n",
      "MRT: Epoch 1 : Covered 12440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.195091 : Minibatch training accuracy= 0.898800\n",
      "MRT: Epoch 1 : Covered 12460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.187423 : Minibatch training accuracy= 0.894261\n",
      "MRT: Epoch 1 : Covered 12480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.708361 : Minibatch training accuracy= 0.883875\n",
      "MRT: Epoch 1 : Covered 12500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.691020 : Minibatch training accuracy= 0.866612\n",
      "MRT: Epoch 1 : Covered 12520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.287745 : Minibatch training accuracy= 0.850953\n",
      "MRT: Epoch 1 : Covered 12540/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.917712 : Minibatch training accuracy= 0.871607\n",
      "MRT: Epoch 1 : Covered 12560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.091597 : Minibatch training accuracy= 0.863048\n",
      "MRT: Epoch 1 : Covered 12580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.058400 : Minibatch training accuracy= 0.871368\n",
      "MRT: Epoch 1 : Covered 12600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.248599 : Minibatch training accuracy= 0.892097\n",
      "MRT: Epoch 1 : Covered 12620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.554969 : Minibatch training accuracy= 0.921392\n",
      "MRT: Epoch 1 : Covered 12640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.021657 : Minibatch training accuracy= 0.854647\n",
      "MRT: Epoch 1 : Covered 12660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.496906 : Minibatch training accuracy= 0.879543\n",
      "MRT: Epoch 1 : Covered 12680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.109543 : Minibatch training accuracy= 0.894602\n",
      "MRT: Epoch 1 : Covered 12700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.845433 : Minibatch training accuracy= 0.877412\n",
      "MRT: Epoch 1 : Covered 12720/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.832748 : Minibatch training accuracy= 0.872343\n",
      "MRT: Epoch 1 : Covered 12740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.858451 : Minibatch training accuracy= 0.863481\n",
      "MRT: Epoch 1 : Covered 12760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.242295 : Minibatch training accuracy= 0.891595\n",
      "MRT: Epoch 1 : Covered 12780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.193507 : Minibatch training accuracy= 0.895555\n",
      "MRT: Epoch 1 : Covered 12800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.257886 : Minibatch training accuracy= 0.902613\n",
      "MRT: Epoch 1 : Covered 12820/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.968204 : Minibatch training accuracy= 0.885823\n",
      "MRT: Epoch 1 : Covered 12840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.056117 : Minibatch training accuracy= 0.881256\n",
      "MRT: Epoch 1 : Covered 12860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.922332 : Minibatch training accuracy= 0.888998\n",
      "MRT: Epoch 1 : Covered 12880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.528795 : Minibatch training accuracy= 0.908062\n",
      "MRT: Epoch 1 : Covered 12900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.070715 : Minibatch training accuracy= 0.866599\n",
      "MRT: Epoch 1 : Covered 12920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.212119 : Minibatch training accuracy= 0.891053\n",
      "MRT: Epoch 1 : Covered 12940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.539361 : Minibatch training accuracy= 0.842424\n",
      "MRT: Epoch 1 : Covered 12960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.947315 : Minibatch training accuracy= 0.879637\n",
      "MRT: Epoch 1 : Covered 12980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.487508 : Minibatch training accuracy= 0.898933\n",
      "MRT: Epoch 1 : Covered 13000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.813791 : Minibatch training accuracy= 0.896869\n",
      "MRT: Epoch 1 : Covered 13020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.753659 : Minibatch training accuracy= 0.866033\n",
      "MRT: Epoch 1 : Covered 13040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.553841 : Minibatch training accuracy= 0.885547\n",
      "MRT: Epoch 1 : Covered 13060/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.453784 : Minibatch training accuracy= 0.840120\n",
      "MRT: Epoch 1 : Covered 13080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.215945 : Minibatch training accuracy= 0.849253\n",
      "MRT: Epoch 1 : Covered 13100/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.498310 : Minibatch training accuracy= 0.873640\n",
      "MRT: Epoch 1 : Covered 13120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.975992 : Minibatch training accuracy= 0.882878\n",
      "MRT: Epoch 1 : Covered 13140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.618514 : Minibatch training accuracy= 0.861316\n",
      "MRT: Epoch 1 : Covered 13160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.023006 : Minibatch training accuracy= 0.852724\n",
      "MRT: Epoch 1 : Covered 13180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.849116 : Minibatch training accuracy= 0.870848\n",
      "MRT: Epoch 1 : Covered 13200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.181584 : Minibatch training accuracy= 0.885148\n",
      "MRT: Epoch 1 : Covered 13220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.658980 : Minibatch training accuracy= 0.902986\n",
      "MRT: Epoch 1 : Covered 13240/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.902877 : Minibatch training accuracy= 0.871993\n",
      "MRT: Epoch 1 : Covered 13260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.785405 : Minibatch training accuracy= 0.903893\n",
      "MRT: Epoch 1 : Covered 13280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.923623 : Minibatch training accuracy= 0.887692\n",
      "MRT: Epoch 1 : Covered 13300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.373019 : Minibatch training accuracy= 0.868241\n",
      "MRT: Epoch 1 : Covered 13320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.025894 : Minibatch training accuracy= 0.890016\n",
      "MRT: Epoch 1 : Covered 13340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.994391 : Minibatch training accuracy= 0.874220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 13360/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.923364 : Minibatch training accuracy= 0.887756\n",
      "MRT: Epoch 1 : Covered 13380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.187935 : Minibatch training accuracy= 0.880417\n",
      "MRT: Epoch 1 : Covered 13400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.421798 : Minibatch training accuracy= 0.856612\n",
      "MRT: Epoch 1 : Covered 13420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.003667 : Minibatch training accuracy= 0.885560\n",
      "MRT: Epoch 1 : Covered 13440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.809800 : Minibatch training accuracy= 0.892110\n",
      "MRT: Epoch 1 : Covered 13460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.551484 : Minibatch training accuracy= 0.888279\n",
      "MRT: Epoch 1 : Covered 13480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.825318 : Minibatch training accuracy= 0.881224\n",
      "MRT: Epoch 1 : Covered 13500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.689401 : Minibatch training accuracy= 0.858064\n",
      "MRT: Epoch 1 : Covered 13520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.825914 : Minibatch training accuracy= 0.897472\n",
      "MRT: Epoch 1 : Covered 13540/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.925766 : Minibatch training accuracy= 0.893466\n",
      "MRT: Epoch 1 : Covered 13560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.090159 : Minibatch training accuracy= 0.854196\n",
      "MRT: Epoch 1 : Covered 13580/90262 : Minibatch Reward Weighted Multisample CE Loss= 0.925237 : Minibatch training accuracy= 0.970650\n",
      "MRT: Epoch 1 : Covered 13600/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.838739 : Minibatch training accuracy= 0.906072\n",
      "MRT: Epoch 1 : Covered 13620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.465091 : Minibatch training accuracy= 0.894023\n",
      "MRT: Epoch 1 : Covered 13640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.877886 : Minibatch training accuracy= 0.863686\n",
      "MRT: Epoch 1 : Covered 13660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.132937 : Minibatch training accuracy= 0.874163\n",
      "MRT: Epoch 1 : Covered 13680/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.940229 : Minibatch training accuracy= 0.884851\n",
      "MRT: Epoch 1 : Covered 13700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.806437 : Minibatch training accuracy= 0.884115\n",
      "MRT: Epoch 1 : Covered 13720/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.693863 : Minibatch training accuracy= 0.894389\n",
      "MRT: Epoch 1 : Covered 13740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.459501 : Minibatch training accuracy= 0.869280\n",
      "MRT: Epoch 1 : Covered 13760/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.765023 : Minibatch training accuracy= 0.900994\n",
      "MRT: Epoch 1 : Covered 13780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.042196 : Minibatch training accuracy= 0.872765\n",
      "MRT: Epoch 1 : Covered 13800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.869773 : Minibatch training accuracy= 0.837717\n",
      "MRT: Epoch 1 : Covered 13820/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.520891 : Minibatch training accuracy= 0.881158\n",
      "MRT: Epoch 1 : Covered 13840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.778346 : Minibatch training accuracy= 0.862497\n",
      "MRT: Epoch 1 : Covered 13860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.237696 : Minibatch training accuracy= 0.896911\n",
      "MRT: Epoch 1 : Covered 13880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.451500 : Minibatch training accuracy= 0.880929\n",
      "MRT: Epoch 1 : Covered 13900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.946325 : Minibatch training accuracy= 0.869642\n",
      "MRT: Epoch 1 : Covered 13920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.159670 : Minibatch training accuracy= 0.885740\n",
      "MRT: Epoch 1 : Covered 13940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.454734 : Minibatch training accuracy= 0.882543\n",
      "MRT: Epoch 1 : Covered 13960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.441885 : Minibatch training accuracy= 0.865085\n",
      "MRT: Epoch 1 : Covered 13980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.298641 : Minibatch training accuracy= 0.905385\n",
      "MRT: Epoch 1 : Covered 14000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.811645 : Minibatch training accuracy= 0.866842\n",
      "MRT: Epoch 1 : Covered 14020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.193445 : Minibatch training accuracy= 0.886813\n",
      "MRT: Epoch 1 : Covered 14040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.918575 : Minibatch training accuracy= 0.872138\n",
      "MRT: Epoch 1 : Covered 14060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.053869 : Minibatch training accuracy= 0.903328\n",
      "MRT: Epoch 1 : Covered 14080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.045321 : Minibatch training accuracy= 0.897122\n",
      "MRT: Epoch 1 : Covered 14100/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.903881 : Minibatch training accuracy= 0.866526\n",
      "MRT: Epoch 1 : Covered 14120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.920871 : Minibatch training accuracy= 0.891761\n",
      "MRT: Epoch 1 : Covered 14140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.805459 : Minibatch training accuracy= 0.901094\n",
      "MRT: Epoch 1 : Covered 14160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.000243 : Minibatch training accuracy= 0.866873\n",
      "MRT: Epoch 1 : Covered 14180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.744096 : Minibatch training accuracy= 0.875086\n",
      "MRT: Epoch 1 : Covered 14200/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.918545 : Minibatch training accuracy= 0.863317\n",
      "MRT: Epoch 1 : Covered 14220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.024255 : Minibatch training accuracy= 0.864644\n",
      "MRT: Epoch 1 : Covered 14240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.089201 : Minibatch training accuracy= 0.904315\n",
      "MRT: Epoch 1 : Covered 14260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.623458 : Minibatch training accuracy= 0.876997\n",
      "MRT: Epoch 1 : Covered 14280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.757061 : Minibatch training accuracy= 0.850857\n",
      "MRT: Epoch 1 : Covered 14300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.868591 : Minibatch training accuracy= 0.869824\n",
      "MRT: Epoch 1 : Covered 14320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.974499 : Minibatch training accuracy= 0.930154\n",
      "MRT: Epoch 1 : Covered 14340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.914697 : Minibatch training accuracy= 0.902582\n",
      "MRT: Epoch 1 : Covered 14360/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.925460 : Minibatch training accuracy= 0.848468\n",
      "MRT: Epoch 1 : Covered 14380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.044495 : Minibatch training accuracy= 0.882986\n",
      "MRT: Epoch 1 : Covered 14400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.902496 : Minibatch training accuracy= 0.870355\n",
      "MRT: Epoch 1 : Covered 14420/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.371872 : Minibatch training accuracy= 0.883531\n",
      "MRT: Epoch 1 : Covered 14440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.734751 : Minibatch training accuracy= 0.884031\n",
      "MRT: Epoch 1 : Covered 14460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.549517 : Minibatch training accuracy= 0.887483\n",
      "MRT: Epoch 1 : Covered 14480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.548931 : Minibatch training accuracy= 0.904865\n",
      "MRT: Epoch 1 : Covered 14500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.150564 : Minibatch training accuracy= 0.896903\n",
      "MRT: Epoch 1 : Covered 14520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.355708 : Minibatch training accuracy= 0.873124\n",
      "MRT: Epoch 1 : Covered 14540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.281847 : Minibatch training accuracy= 0.881894\n",
      "MRT: Epoch 1 : Covered 14560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.509055 : Minibatch training accuracy= 0.898879\n",
      "MRT: Epoch 1 : Covered 14580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.449214 : Minibatch training accuracy= 0.899409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 14600/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.939112 : Minibatch training accuracy= 0.898459\n",
      "MRT: Epoch 1 : Covered 14620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.020968 : Minibatch training accuracy= 0.898114\n",
      "MRT: Epoch 1 : Covered 14640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.798785 : Minibatch training accuracy= 0.865810\n",
      "MRT: Epoch 1 : Covered 14660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.032993 : Minibatch training accuracy= 0.903244\n",
      "MRT: Epoch 1 : Covered 14680/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.769318 : Minibatch training accuracy= 0.872679\n",
      "MRT: Epoch 1 : Covered 14700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.208409 : Minibatch training accuracy= 0.834542\n",
      "MRT: Epoch 1 : Covered 14720/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.857486 : Minibatch training accuracy= 0.873447\n",
      "MRT: Epoch 1 : Covered 14740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.615021 : Minibatch training accuracy= 0.895065\n",
      "MRT: Epoch 1 : Covered 14760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.275670 : Minibatch training accuracy= 0.898452\n",
      "MRT: Epoch 1 : Covered 14780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.086927 : Minibatch training accuracy= 0.885888\n",
      "MRT: Epoch 1 : Covered 14800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.573173 : Minibatch training accuracy= 0.860381\n",
      "MRT: Epoch 1 : Covered 14820/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.670460 : Minibatch training accuracy= 0.873026\n",
      "MRT: Epoch 1 : Covered 14840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.167464 : Minibatch training accuracy= 0.870422\n",
      "MRT: Epoch 1 : Covered 14860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.807902 : Minibatch training accuracy= 0.895590\n",
      "MRT: Epoch 1 : Covered 14880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.931741 : Minibatch training accuracy= 0.904382\n",
      "MRT: Epoch 1 : Covered 14900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.815475 : Minibatch training accuracy= 0.856962\n",
      "MRT: Epoch 1 : Covered 14920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.852508 : Minibatch training accuracy= 0.911743\n",
      "MRT: Epoch 1 : Covered 14940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.875837 : Minibatch training accuracy= 0.860057\n",
      "MRT: Epoch 1 : Covered 14960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.789801 : Minibatch training accuracy= 0.875261\n",
      "MRT: Epoch 1 : Covered 14980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.366830 : Minibatch training accuracy= 0.889500\n",
      "MRT: Epoch 1 : Covered 15000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.977913 : Minibatch training accuracy= 0.880146\n",
      "MRT: Epoch 1 : Covered 15020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.799312 : Minibatch training accuracy= 0.900171\n",
      "MRT: Epoch 1 : Covered 15040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.940313 : Minibatch training accuracy= 0.861744\n",
      "MRT: Epoch 1 : Covered 15060/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.766005 : Minibatch training accuracy= 0.866430\n",
      "MRT: Epoch 1 : Covered 15080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.655115 : Minibatch training accuracy= 0.860525\n",
      "MRT: Epoch 1 : Covered 15100/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.491376 : Minibatch training accuracy= 0.837858\n",
      "MRT: Epoch 1 : Covered 15120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.702753 : Minibatch training accuracy= 0.867668\n",
      "MRT: Epoch 1 : Covered 15140/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.762794 : Minibatch training accuracy= 0.882451\n",
      "MRT: Epoch 1 : Covered 15160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.336236 : Minibatch training accuracy= 0.874096\n",
      "MRT: Epoch 1 : Covered 15180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.584975 : Minibatch training accuracy= 0.876083\n",
      "MRT: Epoch 1 : Covered 15200/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.911064 : Minibatch training accuracy= 0.883338\n",
      "MRT: Epoch 1 : Covered 15220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.345618 : Minibatch training accuracy= 0.888910\n",
      "MRT: Epoch 1 : Covered 15240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.068563 : Minibatch training accuracy= 0.891137\n",
      "MRT: Epoch 1 : Covered 15260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.108869 : Minibatch training accuracy= 0.872158\n",
      "MRT: Epoch 1 : Covered 15280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.390050 : Minibatch training accuracy= 0.873441\n",
      "MRT: Epoch 1 : Covered 15300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.189946 : Minibatch training accuracy= 0.890433\n",
      "MRT: Epoch 1 : Covered 15320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.784069 : Minibatch training accuracy= 0.907868\n",
      "MRT: Epoch 1 : Covered 15340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.822918 : Minibatch training accuracy= 0.868271\n",
      "MRT: Epoch 1 : Covered 15360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.150924 : Minibatch training accuracy= 0.882909\n",
      "MRT: Epoch 1 : Covered 15380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.769532 : Minibatch training accuracy= 0.903751\n",
      "MRT: Epoch 1 : Covered 15400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.877773 : Minibatch training accuracy= 0.888024\n",
      "MRT: Epoch 1 : Covered 15420/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.451290 : Minibatch training accuracy= 0.921808\n",
      "MRT: Epoch 1 : Covered 15440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.675148 : Minibatch training accuracy= 0.874770\n",
      "MRT: Epoch 1 : Covered 15460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.420961 : Minibatch training accuracy= 0.901365\n",
      "MRT: Epoch 1 : Covered 15480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.810569 : Minibatch training accuracy= 0.895312\n",
      "MRT: Epoch 1 : Covered 15500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.856296 : Minibatch training accuracy= 0.886909\n",
      "MRT: Epoch 1 : Covered 15520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.806911 : Minibatch training accuracy= 0.876159\n",
      "MRT: Epoch 1 : Covered 15540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.452913 : Minibatch training accuracy= 0.942012\n",
      "MRT: Epoch 1 : Covered 15560/90262 : Minibatch Reward Weighted Multisample CE Loss= 7.280888 : Minibatch training accuracy= 0.375979\n",
      "MRT: Epoch 1 : Covered 15580/90262 : Minibatch Reward Weighted Multisample CE Loss= 6.848872 : Minibatch training accuracy= 0.568092\n",
      "MRT: Epoch 1 : Covered 15600/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.204794 : Minibatch training accuracy= 0.782246\n",
      "MRT: Epoch 1 : Covered 15620/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.418338 : Minibatch training accuracy= 0.886353\n",
      "MRT: Epoch 1 : Covered 15640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.411494 : Minibatch training accuracy= 0.900244\n",
      "MRT: Epoch 1 : Covered 15660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.816571 : Minibatch training accuracy= 0.852247\n",
      "MRT: Epoch 1 : Covered 15680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.313630 : Minibatch training accuracy= 0.902847\n",
      "MRT: Epoch 1 : Covered 15700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.919674 : Minibatch training accuracy= 0.913197\n",
      "MRT: Epoch 1 : Covered 15720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.729765 : Minibatch training accuracy= 0.899870\n",
      "MRT: Epoch 1 : Covered 15740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.633302 : Minibatch training accuracy= 0.878081\n",
      "MRT: Epoch 1 : Covered 15760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.350826 : Minibatch training accuracy= 0.906051\n",
      "MRT: Epoch 1 : Covered 15780/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.849656 : Minibatch training accuracy= 0.876763\n",
      "MRT: Epoch 1 : Covered 15800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.922262 : Minibatch training accuracy= 0.889777\n",
      "MRT: Epoch 1 : Covered 15820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.319297 : Minibatch training accuracy= 0.894783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 15840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.417931 : Minibatch training accuracy= 0.871910\n",
      "MRT: Epoch 1 : Covered 15860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.905245 : Minibatch training accuracy= 0.929450\n",
      "MRT: Epoch 1 : Covered 15880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.698785 : Minibatch training accuracy= 0.811897\n",
      "MRT: Epoch 1 : Covered 15900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.637210 : Minibatch training accuracy= 0.910058\n",
      "MRT: Epoch 1 : Covered 15920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.782255 : Minibatch training accuracy= 0.915641\n",
      "MRT: Epoch 1 : Covered 15940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.813956 : Minibatch training accuracy= 0.868650\n",
      "MRT: Epoch 1 : Covered 15960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.400472 : Minibatch training accuracy= 0.912312\n",
      "MRT: Epoch 1 : Covered 15980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.269700 : Minibatch training accuracy= 0.909146\n",
      "MRT: Epoch 1 : Covered 16000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.711629 : Minibatch training accuracy= 0.920962\n",
      "MRT: Epoch 1 : Covered 16020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.833867 : Minibatch training accuracy= 0.891547\n",
      "MRT: Epoch 1 : Covered 16040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.544657 : Minibatch training accuracy= 0.924370\n",
      "MRT: Epoch 1 : Covered 16060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.542365 : Minibatch training accuracy= 0.912721\n",
      "MRT: Epoch 1 : Covered 16080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.608396 : Minibatch training accuracy= 0.892385\n",
      "MRT: Epoch 1 : Covered 16100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.983797 : Minibatch training accuracy= 0.922793\n",
      "MRT: Epoch 1 : Covered 16120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.081720 : Minibatch training accuracy= 0.936882\n",
      "MRT: Epoch 1 : Covered 16140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.660153 : Minibatch training accuracy= 0.911213\n",
      "MRT: Epoch 1 : Covered 16160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.474956 : Minibatch training accuracy= 0.934490\n",
      "MRT: Epoch 1 : Covered 16180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.854264 : Minibatch training accuracy= 0.875507\n",
      "MRT: Epoch 1 : Covered 16200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.001130 : Minibatch training accuracy= 0.887701\n",
      "MRT: Epoch 1 : Covered 16220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.831388 : Minibatch training accuracy= 0.890088\n",
      "MRT: Epoch 1 : Covered 16240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.006200 : Minibatch training accuracy= 0.890675\n",
      "MRT: Epoch 1 : Covered 16260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.987381 : Minibatch training accuracy= 0.917782\n",
      "MRT: Epoch 1 : Covered 16280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.072708 : Minibatch training accuracy= 0.888893\n",
      "MRT: Epoch 1 : Covered 16300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.029559 : Minibatch training accuracy= 0.857290\n",
      "MRT: Epoch 1 : Covered 16320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.813102 : Minibatch training accuracy= 0.876900\n",
      "MRT: Epoch 1 : Covered 16340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.934551 : Minibatch training accuracy= 0.872415\n",
      "MRT: Epoch 1 : Covered 16360/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.898612 : Minibatch training accuracy= 0.887083\n",
      "MRT: Epoch 1 : Covered 16380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.746494 : Minibatch training accuracy= 0.843474\n",
      "MRT: Epoch 1 : Covered 16400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.073868 : Minibatch training accuracy= 0.878307\n",
      "MRT: Epoch 1 : Covered 16420/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.860336 : Minibatch training accuracy= 0.883151\n",
      "MRT: Epoch 1 : Covered 16440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.970251 : Minibatch training accuracy= 0.862653\n",
      "MRT: Epoch 1 : Covered 16460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.358878 : Minibatch training accuracy= 0.881825\n",
      "MRT: Epoch 1 : Covered 16480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.374765 : Minibatch training accuracy= 0.860695\n",
      "MRT: Epoch 1 : Covered 16500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.389905 : Minibatch training accuracy= 0.912763\n",
      "MRT: Epoch 1 : Covered 16520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.637508 : Minibatch training accuracy= 0.881159\n",
      "MRT: Epoch 1 : Covered 16540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.069404 : Minibatch training accuracy= 0.851246\n",
      "MRT: Epoch 1 : Covered 16560/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.873783 : Minibatch training accuracy= 0.879379\n",
      "MRT: Epoch 1 : Covered 16580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.077101 : Minibatch training accuracy= 0.912204\n",
      "MRT: Epoch 1 : Covered 16600/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.915211 : Minibatch training accuracy= 0.897594\n",
      "MRT: Epoch 1 : Covered 16620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.453582 : Minibatch training accuracy= 0.890965\n",
      "MRT: Epoch 1 : Covered 16640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.931748 : Minibatch training accuracy= 0.886496\n",
      "MRT: Epoch 1 : Covered 16660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.160414 : Minibatch training accuracy= 0.891543\n",
      "MRT: Epoch 1 : Covered 16680/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.704809 : Minibatch training accuracy= 0.839335\n",
      "MRT: Epoch 1 : Covered 16700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.748681 : Minibatch training accuracy= 0.879282\n",
      "MRT: Epoch 1 : Covered 16720/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.829182 : Minibatch training accuracy= 0.855584\n",
      "MRT: Epoch 1 : Covered 16740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.937225 : Minibatch training accuracy= 0.886355\n",
      "MRT: Epoch 1 : Covered 16760/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.878010 : Minibatch training accuracy= 0.880628\n",
      "MRT: Epoch 1 : Covered 16780/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.947905 : Minibatch training accuracy= 0.843424\n",
      "MRT: Epoch 1 : Covered 16800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.387908 : Minibatch training accuracy= 0.850326\n",
      "MRT: Epoch 1 : Covered 16820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.734557 : Minibatch training accuracy= 0.845360\n",
      "MRT: Epoch 1 : Covered 16840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.453842 : Minibatch training accuracy= 0.899679\n",
      "MRT: Epoch 1 : Covered 16860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.552711 : Minibatch training accuracy= 0.844701\n",
      "MRT: Epoch 1 : Covered 16880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.253249 : Minibatch training accuracy= 0.861696\n",
      "MRT: Epoch 1 : Covered 16900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.790041 : Minibatch training accuracy= 0.854057\n",
      "MRT: Epoch 1 : Covered 16920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.926111 : Minibatch training accuracy= 0.893056\n",
      "MRT: Epoch 1 : Covered 16940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.748159 : Minibatch training accuracy= 0.867838\n",
      "MRT: Epoch 1 : Covered 16960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.265017 : Minibatch training accuracy= 0.867542\n",
      "MRT: Epoch 1 : Covered 16980/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.791219 : Minibatch training accuracy= 0.865475\n",
      "MRT: Epoch 1 : Covered 17000/90262 : Minibatch Reward Weighted Multisample CE Loss= 14.912247 : Minibatch training accuracy= 0.195361\n",
      "MRT: Epoch 1 : Covered 17020/90262 : Minibatch Reward Weighted Multisample CE Loss= 13.870468 : Minibatch training accuracy= 0.183739\n",
      "MRT: Epoch 1 : Covered 17040/90262 : Minibatch Reward Weighted Multisample CE Loss= 11.498764 : Minibatch training accuracy= 0.210888\n",
      "MRT: Epoch 1 : Covered 17060/90262 : Minibatch Reward Weighted Multisample CE Loss= 8.203798 : Minibatch training accuracy= 0.426129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 17080/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.170173 : Minibatch training accuracy= 0.591573\n",
      "MRT: Epoch 1 : Covered 17100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.309198 : Minibatch training accuracy= 0.868807\n",
      "MRT: Epoch 1 : Covered 17120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.509793 : Minibatch training accuracy= 0.893720\n",
      "MRT: Epoch 1 : Covered 17140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.274136 : Minibatch training accuracy= 0.902849\n",
      "MRT: Epoch 1 : Covered 17160/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.830412 : Minibatch training accuracy= 0.847327\n",
      "MRT: Epoch 1 : Covered 17180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.362284 : Minibatch training accuracy= 0.855001\n",
      "MRT: Epoch 1 : Covered 17200/90262 : Minibatch Reward Weighted Multisample CE Loss= 16.705389 : Minibatch training accuracy= 0.553416\n",
      "MRT: Epoch 1 : Covered 17220/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.583005 : Minibatch training accuracy= 0.894942\n",
      "MRT: Epoch 1 : Covered 17240/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.370088 : Minibatch training accuracy= 0.907238\n",
      "MRT: Epoch 1 : Covered 17260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.928469 : Minibatch training accuracy= 0.867564\n",
      "MRT: Epoch 1 : Covered 17280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.945563 : Minibatch training accuracy= 0.874919\n",
      "MRT: Epoch 1 : Covered 17300/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.271454 : Minibatch training accuracy= 0.851588\n",
      "MRT: Epoch 1 : Covered 17320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.891332 : Minibatch training accuracy= 0.881990\n",
      "MRT: Epoch 1 : Covered 17340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.507025 : Minibatch training accuracy= 0.872108\n",
      "MRT: Epoch 1 : Covered 17360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.299739 : Minibatch training accuracy= 0.897782\n",
      "MRT: Epoch 1 : Covered 17380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.585576 : Minibatch training accuracy= 0.873406\n",
      "MRT: Epoch 1 : Covered 17400/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.271873 : Minibatch training accuracy= 0.876411\n",
      "MRT: Epoch 1 : Covered 17420/90262 : Minibatch Reward Weighted Multisample CE Loss= 13.157112 : Minibatch training accuracy= 0.815069\n",
      "MRT: Epoch 1 : Covered 17440/90262 : Minibatch Reward Weighted Multisample CE Loss= 8.907232 : Minibatch training accuracy= 0.692457\n",
      "MRT: Epoch 1 : Covered 17460/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.245914 : Minibatch training accuracy= 0.921114\n",
      "MRT: Epoch 1 : Covered 17480/90262 : Minibatch Reward Weighted Multisample CE Loss= 6.947810 : Minibatch training accuracy= 0.539625\n",
      "MRT: Epoch 1 : Covered 17500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.046715 : Minibatch training accuracy= 0.887621\n",
      "MRT: Epoch 1 : Covered 17520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.035040 : Minibatch training accuracy= 0.870718\n",
      "MRT: Epoch 1 : Covered 17540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.301427 : Minibatch training accuracy= 0.883226\n",
      "MRT: Epoch 1 : Covered 17560/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.821565 : Minibatch training accuracy= 0.898220\n",
      "MRT: Epoch 1 : Covered 17580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.914135 : Minibatch training accuracy= 0.871012\n",
      "MRT: Epoch 1 : Covered 17600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.186767 : Minibatch training accuracy= 0.869892\n",
      "MRT: Epoch 1 : Covered 17620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.761924 : Minibatch training accuracy= 0.888344\n",
      "MRT: Epoch 1 : Covered 17640/90262 : Minibatch Reward Weighted Multisample CE Loss= 13.136981 : Minibatch training accuracy= 0.180842\n",
      "MRT: Epoch 1 : Covered 17660/90262 : Minibatch Reward Weighted Multisample CE Loss= 13.511953 : Minibatch training accuracy= 0.243722\n",
      "MRT: Epoch 1 : Covered 17680/90262 : Minibatch Reward Weighted Multisample CE Loss= 9.111502 : Minibatch training accuracy= 0.275595\n",
      "MRT: Epoch 1 : Covered 17700/90262 : Minibatch Reward Weighted Multisample CE Loss= 8.090417 : Minibatch training accuracy= 0.439580\n",
      "MRT: Epoch 1 : Covered 17720/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.425317 : Minibatch training accuracy= 0.676410\n",
      "MRT: Epoch 1 : Covered 17740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.354722 : Minibatch training accuracy= 0.837030\n",
      "MRT: Epoch 1 : Covered 17760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.477441 : Minibatch training accuracy= 0.871596\n",
      "MRT: Epoch 1 : Covered 17780/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.018020 : Minibatch training accuracy= 0.877078\n",
      "MRT: Epoch 1 : Covered 17800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.921310 : Minibatch training accuracy= 0.867032\n",
      "MRT: Epoch 1 : Covered 17820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.289883 : Minibatch training accuracy= 0.895497\n",
      "MRT: Epoch 1 : Covered 17840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.443386 : Minibatch training accuracy= 0.896833\n",
      "MRT: Epoch 1 : Covered 17860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.158643 : Minibatch training accuracy= 0.879292\n",
      "MRT: Epoch 1 : Covered 17880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.500557 : Minibatch training accuracy= 0.863223\n",
      "MRT: Epoch 1 : Covered 17900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.365873 : Minibatch training accuracy= 0.854276\n",
      "MRT: Epoch 1 : Covered 17920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.802039 : Minibatch training accuracy= 0.873057\n",
      "MRT: Epoch 1 : Covered 17940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.503106 : Minibatch training accuracy= 0.887280\n",
      "MRT: Epoch 1 : Covered 17960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.167370 : Minibatch training accuracy= 0.856275\n",
      "MRT: Epoch 1 : Covered 17980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.317253 : Minibatch training accuracy= 0.878268\n",
      "MRT: Epoch 1 : Covered 18000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.477271 : Minibatch training accuracy= 0.885549\n",
      "MRT: Epoch 1 : Covered 18020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.132949 : Minibatch training accuracy= 0.865920\n",
      "MRT: Epoch 1 : Covered 18040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.399716 : Minibatch training accuracy= 0.858518\n",
      "MRT: Epoch 1 : Covered 18060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.071395 : Minibatch training accuracy= 0.858343\n",
      "MRT: Epoch 1 : Covered 18080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.131264 : Minibatch training accuracy= 0.887665\n",
      "MRT: Epoch 1 : Covered 18100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.033670 : Minibatch training accuracy= 0.877892\n",
      "MRT: Epoch 1 : Covered 18120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.814992 : Minibatch training accuracy= 0.889812\n",
      "MRT: Epoch 1 : Covered 18140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.160931 : Minibatch training accuracy= 0.828931\n",
      "MRT: Epoch 1 : Covered 18160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.180972 : Minibatch training accuracy= 0.890066\n",
      "MRT: Epoch 1 : Covered 18180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.284859 : Minibatch training accuracy= 0.892625\n",
      "MRT: Epoch 1 : Covered 18200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.729705 : Minibatch training accuracy= 0.903985\n",
      "MRT: Epoch 1 : Covered 18220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.504848 : Minibatch training accuracy= 0.861323\n",
      "MRT: Epoch 1 : Covered 18240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.373838 : Minibatch training accuracy= 0.880476\n",
      "MRT: Epoch 1 : Covered 18260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.423672 : Minibatch training accuracy= 0.871818\n",
      "MRT: Epoch 1 : Covered 18280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.382632 : Minibatch training accuracy= 0.900293\n",
      "MRT: Epoch 1 : Covered 18300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.108128 : Minibatch training accuracy= 0.875214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 18320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.164228 : Minibatch training accuracy= 0.890201\n",
      "MRT: Epoch 1 : Covered 18340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.204881 : Minibatch training accuracy= 0.844526\n",
      "MRT: Epoch 1 : Covered 18360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.418771 : Minibatch training accuracy= 0.899379\n",
      "MRT: Epoch 1 : Covered 18380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.631484 : Minibatch training accuracy= 0.858298\n",
      "MRT: Epoch 1 : Covered 18400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.589750 : Minibatch training accuracy= 0.892784\n",
      "MRT: Epoch 1 : Covered 18420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.010937 : Minibatch training accuracy= 0.854895\n",
      "MRT: Epoch 1 : Covered 18440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.839842 : Minibatch training accuracy= 0.881945\n",
      "MRT: Epoch 1 : Covered 18460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.306622 : Minibatch training accuracy= 0.855350\n",
      "MRT: Epoch 1 : Covered 18480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.956247 : Minibatch training accuracy= 0.840388\n",
      "MRT: Epoch 1 : Covered 18500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.389285 : Minibatch training accuracy= 0.878147\n",
      "MRT: Epoch 1 : Covered 18520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.694879 : Minibatch training accuracy= 0.887947\n",
      "MRT: Epoch 1 : Covered 18540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.074418 : Minibatch training accuracy= 0.875416\n",
      "MRT: Epoch 1 : Covered 18560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.174960 : Minibatch training accuracy= 0.878032\n",
      "MRT: Epoch 1 : Covered 18580/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.931812 : Minibatch training accuracy= 0.900018\n",
      "MRT: Epoch 1 : Covered 18600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.138669 : Minibatch training accuracy= 0.898401\n",
      "MRT: Epoch 1 : Covered 18620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.001686 : Minibatch training accuracy= 0.888982\n",
      "MRT: Epoch 1 : Covered 18640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.523867 : Minibatch training accuracy= 0.871196\n",
      "MRT: Epoch 1 : Covered 18660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.802628 : Minibatch training accuracy= 0.867922\n",
      "MRT: Epoch 1 : Covered 18680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.128909 : Minibatch training accuracy= 0.875703\n",
      "MRT: Epoch 1 : Covered 18700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.900920 : Minibatch training accuracy= 0.868780\n",
      "MRT: Epoch 1 : Covered 18720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.230933 : Minibatch training accuracy= 0.865606\n",
      "MRT: Epoch 1 : Covered 18740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.557168 : Minibatch training accuracy= 0.903994\n",
      "MRT: Epoch 1 : Covered 18760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.466798 : Minibatch training accuracy= 0.891264\n",
      "MRT: Epoch 1 : Covered 18780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.662949 : Minibatch training accuracy= 0.909340\n",
      "MRT: Epoch 1 : Covered 18800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.109231 : Minibatch training accuracy= 0.900382\n",
      "MRT: Epoch 1 : Covered 18820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.426302 : Minibatch training accuracy= 0.903192\n",
      "MRT: Epoch 1 : Covered 18840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.800043 : Minibatch training accuracy= 0.899864\n",
      "MRT: Epoch 1 : Covered 18860/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.056193 : Minibatch training accuracy= 0.897397\n",
      "MRT: Epoch 1 : Covered 18880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.557535 : Minibatch training accuracy= 0.893286\n",
      "MRT: Epoch 1 : Covered 18900/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.156918 : Minibatch training accuracy= 0.903035\n",
      "MRT: Epoch 1 : Covered 18920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.827801 : Minibatch training accuracy= 0.909062\n",
      "MRT: Epoch 1 : Covered 18940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.068164 : Minibatch training accuracy= 0.865639\n",
      "MRT: Epoch 1 : Covered 18960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.450815 : Minibatch training accuracy= 0.914629\n",
      "MRT: Epoch 1 : Covered 18980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.743438 : Minibatch training accuracy= 0.917060\n",
      "MRT: Epoch 1 : Covered 19000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.482372 : Minibatch training accuracy= 0.885333\n",
      "MRT: Epoch 1 : Covered 19020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.797621 : Minibatch training accuracy= 0.888275\n",
      "MRT: Epoch 1 : Covered 19040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.537247 : Minibatch training accuracy= 0.871436\n",
      "MRT: Epoch 1 : Covered 19060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.058833 : Minibatch training accuracy= 0.862259\n",
      "MRT: Epoch 1 : Covered 19080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.727699 : Minibatch training accuracy= 0.876425\n",
      "MRT: Epoch 1 : Covered 19100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.079778 : Minibatch training accuracy= 0.912269\n",
      "MRT: Epoch 1 : Covered 19120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.496199 : Minibatch training accuracy= 0.862032\n",
      "MRT: Epoch 1 : Covered 19140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.523450 : Minibatch training accuracy= 0.889180\n",
      "MRT: Epoch 1 : Covered 19160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.607221 : Minibatch training accuracy= 0.881074\n",
      "MRT: Epoch 1 : Covered 19180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.325661 : Minibatch training accuracy= 0.880691\n",
      "MRT: Epoch 1 : Covered 19200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.420440 : Minibatch training accuracy= 0.880509\n",
      "MRT: Epoch 1 : Covered 19220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.511468 : Minibatch training accuracy= 0.865495\n",
      "MRT: Epoch 1 : Covered 19240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.544406 : Minibatch training accuracy= 0.863384\n",
      "MRT: Epoch 1 : Covered 19260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.918329 : Minibatch training accuracy= 0.902717\n",
      "MRT: Epoch 1 : Covered 19280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.127478 : Minibatch training accuracy= 0.862092\n",
      "MRT: Epoch 1 : Covered 19300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.081241 : Minibatch training accuracy= 0.889883\n",
      "MRT: Epoch 1 : Covered 19320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.978117 : Minibatch training accuracy= 0.887871\n",
      "MRT: Epoch 1 : Covered 19340/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.973119 : Minibatch training accuracy= 0.857980\n",
      "MRT: Epoch 1 : Covered 19360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.077625 : Minibatch training accuracy= 0.891631\n",
      "MRT: Epoch 1 : Covered 19380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.060050 : Minibatch training accuracy= 0.864186\n",
      "MRT: Epoch 1 : Covered 19400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.865412 : Minibatch training accuracy= 0.867091\n",
      "MRT: Epoch 1 : Covered 19420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.063816 : Minibatch training accuracy= 0.885969\n",
      "MRT: Epoch 1 : Covered 19440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.183830 : Minibatch training accuracy= 0.856966\n",
      "MRT: Epoch 1 : Covered 19460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.813941 : Minibatch training accuracy= 0.885108\n",
      "MRT: Epoch 1 : Covered 19480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.252934 : Minibatch training accuracy= 0.892268\n",
      "MRT: Epoch 1 : Covered 19500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.119409 : Minibatch training accuracy= 0.877743\n",
      "MRT: Epoch 1 : Covered 19520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.043023 : Minibatch training accuracy= 0.853390\n",
      "MRT: Epoch 1 : Covered 19540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.162858 : Minibatch training accuracy= 0.861891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 19560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.191751 : Minibatch training accuracy= 0.867942\n",
      "MRT: Epoch 1 : Covered 19580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.161050 : Minibatch training accuracy= 0.897469\n",
      "MRT: Epoch 1 : Covered 19600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.422069 : Minibatch training accuracy= 0.871312\n",
      "MRT: Epoch 1 : Covered 19620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.058281 : Minibatch training accuracy= 0.887586\n",
      "MRT: Epoch 1 : Covered 19640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.478205 : Minibatch training accuracy= 0.880466\n",
      "MRT: Epoch 1 : Covered 19660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.785827 : Minibatch training accuracy= 0.872568\n",
      "MRT: Epoch 1 : Covered 19680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.108835 : Minibatch training accuracy= 0.888208\n",
      "MRT: Epoch 1 : Covered 19700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.770993 : Minibatch training accuracy= 0.920293\n",
      "MRT: Epoch 1 : Covered 19720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.391338 : Minibatch training accuracy= 0.908847\n",
      "MRT: Epoch 1 : Covered 19740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.475964 : Minibatch training accuracy= 0.898391\n",
      "MRT: Epoch 1 : Covered 19760/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.907103 : Minibatch training accuracy= 0.848844\n",
      "MRT: Epoch 1 : Covered 19780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.775059 : Minibatch training accuracy= 0.882431\n",
      "MRT: Epoch 1 : Covered 19800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.604234 : Minibatch training accuracy= 0.882746\n",
      "MRT: Epoch 1 : Covered 19820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.269383 : Minibatch training accuracy= 0.877541\n",
      "MRT: Epoch 1 : Covered 19840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.747308 : Minibatch training accuracy= 0.880393\n",
      "MRT: Epoch 1 : Covered 19860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.053369 : Minibatch training accuracy= 0.889409\n",
      "MRT: Epoch 1 : Covered 19880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.896945 : Minibatch training accuracy= 0.864457\n",
      "MRT: Epoch 1 : Covered 19900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.933694 : Minibatch training accuracy= 0.893506\n",
      "MRT: Epoch 1 : Covered 19920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.633914 : Minibatch training accuracy= 0.897387\n",
      "MRT: Epoch 1 : Covered 19940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.274534 : Minibatch training accuracy= 0.887074\n",
      "MRT: Epoch 1 : Covered 19960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.251148 : Minibatch training accuracy= 0.885398\n",
      "MRT: Epoch 1 : Covered 19980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.190913 : Minibatch training accuracy= 0.866296\n",
      "MRT: Epoch 1 : Covered 20000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.176553 : Minibatch training accuracy= 0.871162\n",
      "MRT: Epoch 1 : Covered 20020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.899783 : Minibatch training accuracy= 0.869317\n",
      "MRT: Epoch 1 : Covered 20040/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.906660 : Minibatch training accuracy= 0.909074\n",
      "MRT: Epoch 1 : Covered 20060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.298893 : Minibatch training accuracy= 0.901442\n",
      "MRT: Epoch 1 : Covered 20080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.539402 : Minibatch training accuracy= 0.886100\n",
      "MRT: Epoch 1 : Covered 20100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.879879 : Minibatch training accuracy= 0.885922\n",
      "MRT: Epoch 1 : Covered 20120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.962123 : Minibatch training accuracy= 0.927785\n",
      "MRT: Epoch 1 : Covered 20140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.181039 : Minibatch training accuracy= 0.873926\n",
      "MRT: Epoch 1 : Covered 20160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.666892 : Minibatch training accuracy= 0.869477\n",
      "MRT: Epoch 1 : Covered 20180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.216977 : Minibatch training accuracy= 0.886914\n",
      "MRT: Epoch 1 : Covered 20200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.742904 : Minibatch training accuracy= 0.874783\n",
      "MRT: Epoch 1 : Covered 20220/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.305204 : Minibatch training accuracy= 0.905624\n",
      "MRT: Epoch 1 : Covered 20240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.496616 : Minibatch training accuracy= 0.900804\n",
      "MRT: Epoch 1 : Covered 20260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.254622 : Minibatch training accuracy= 0.884725\n",
      "MRT: Epoch 1 : Covered 20280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.072403 : Minibatch training accuracy= 0.890302\n",
      "MRT: Epoch 1 : Covered 20300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.819044 : Minibatch training accuracy= 0.887836\n",
      "MRT: Epoch 1 : Covered 20320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.031844 : Minibatch training accuracy= 0.893756\n",
      "MRT: Epoch 1 : Covered 20340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.068244 : Minibatch training accuracy= 0.904843\n",
      "MRT: Epoch 1 : Covered 20360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.523051 : Minibatch training accuracy= 0.913778\n",
      "MRT: Epoch 1 : Covered 20380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.611673 : Minibatch training accuracy= 0.886732\n",
      "MRT: Epoch 1 : Covered 20400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.065613 : Minibatch training accuracy= 0.875004\n",
      "MRT: Epoch 1 : Covered 20420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.922802 : Minibatch training accuracy= 0.917872\n",
      "MRT: Epoch 1 : Covered 20440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.392922 : Minibatch training accuracy= 0.888641\n",
      "MRT: Epoch 1 : Covered 20460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.215167 : Minibatch training accuracy= 0.893004\n",
      "MRT: Epoch 1 : Covered 20480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.716349 : Minibatch training accuracy= 0.878689\n",
      "MRT: Epoch 1 : Covered 20500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.498168 : Minibatch training accuracy= 0.847014\n",
      "MRT: Epoch 1 : Covered 20520/90262 : Minibatch Reward Weighted Multisample CE Loss= 7.922481 : Minibatch training accuracy= 0.791808\n",
      "MRT: Epoch 1 : Covered 20540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.347007 : Minibatch training accuracy= 0.897696\n",
      "MRT: Epoch 1 : Covered 20560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.045048 : Minibatch training accuracy= 0.896873\n",
      "MRT: Epoch 1 : Covered 20580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.880501 : Minibatch training accuracy= 0.868306\n",
      "MRT: Epoch 1 : Covered 20600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.554915 : Minibatch training accuracy= 0.867615\n",
      "MRT: Epoch 1 : Covered 20620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.865215 : Minibatch training accuracy= 0.901925\n",
      "MRT: Epoch 1 : Covered 20640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.430085 : Minibatch training accuracy= 0.888594\n",
      "MRT: Epoch 1 : Covered 20660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.270097 : Minibatch training accuracy= 0.861188\n",
      "MRT: Epoch 1 : Covered 20680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.434202 : Minibatch training accuracy= 0.876937\n",
      "MRT: Epoch 1 : Covered 20700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.483871 : Minibatch training accuracy= 0.894521\n",
      "MRT: Epoch 1 : Covered 20720/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.386598 : Minibatch training accuracy= 0.908859\n",
      "MRT: Epoch 1 : Covered 20740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.457818 : Minibatch training accuracy= 0.886124\n",
      "MRT: Epoch 1 : Covered 20760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.346465 : Minibatch training accuracy= 0.893795\n",
      "MRT: Epoch 1 : Covered 20780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.652299 : Minibatch training accuracy= 0.883144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 20800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.803038 : Minibatch training accuracy= 0.894667\n",
      "MRT: Epoch 1 : Covered 20820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.367995 : Minibatch training accuracy= 0.898296\n",
      "MRT: Epoch 1 : Covered 20840/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.147532 : Minibatch training accuracy= 0.913367\n",
      "MRT: Epoch 1 : Covered 20860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.515331 : Minibatch training accuracy= 0.872743\n",
      "MRT: Epoch 1 : Covered 20880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.089327 : Minibatch training accuracy= 0.859505\n",
      "MRT: Epoch 1 : Covered 20900/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.993874 : Minibatch training accuracy= 0.880461\n",
      "MRT: Epoch 1 : Covered 20920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.300368 : Minibatch training accuracy= 0.906936\n",
      "MRT: Epoch 1 : Covered 20940/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.695415 : Minibatch training accuracy= 0.864871\n",
      "MRT: Epoch 1 : Covered 20960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.631400 : Minibatch training accuracy= 0.906941\n",
      "MRT: Epoch 1 : Covered 20980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.052478 : Minibatch training accuracy= 0.875102\n",
      "MRT: Epoch 1 : Covered 21000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.319521 : Minibatch training accuracy= 0.899467\n",
      "MRT: Epoch 1 : Covered 21020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.084356 : Minibatch training accuracy= 0.886445\n",
      "MRT: Epoch 1 : Covered 21040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.281133 : Minibatch training accuracy= 0.891338\n",
      "MRT: Epoch 1 : Covered 21060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.816938 : Minibatch training accuracy= 0.894190\n",
      "MRT: Epoch 1 : Covered 21080/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.342124 : Minibatch training accuracy= 0.859262\n",
      "MRT: Epoch 1 : Covered 21100/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.262291 : Minibatch training accuracy= 0.886763\n",
      "MRT: Epoch 1 : Covered 21120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.713160 : Minibatch training accuracy= 0.908661\n",
      "MRT: Epoch 1 : Covered 21140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.900697 : Minibatch training accuracy= 0.889407\n",
      "MRT: Epoch 1 : Covered 21160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.122375 : Minibatch training accuracy= 0.896838\n",
      "MRT: Epoch 1 : Covered 21180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.919056 : Minibatch training accuracy= 0.891488\n",
      "MRT: Epoch 1 : Covered 21200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.280857 : Minibatch training accuracy= 0.881223\n",
      "MRT: Epoch 1 : Covered 21220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.658551 : Minibatch training accuracy= 0.897380\n",
      "MRT: Epoch 1 : Covered 21240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.513931 : Minibatch training accuracy= 0.896081\n",
      "MRT: Epoch 1 : Covered 21260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.900021 : Minibatch training accuracy= 0.890014\n",
      "MRT: Epoch 1 : Covered 21280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.697194 : Minibatch training accuracy= 0.912375\n",
      "MRT: Epoch 1 : Covered 21300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.797235 : Minibatch training accuracy= 0.850397\n",
      "MRT: Epoch 1 : Covered 21320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.407318 : Minibatch training accuracy= 0.904995\n",
      "MRT: Epoch 1 : Covered 21340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.910276 : Minibatch training accuracy= 0.899738\n",
      "MRT: Epoch 1 : Covered 21360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.443555 : Minibatch training accuracy= 0.902833\n",
      "MRT: Epoch 1 : Covered 21380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.889712 : Minibatch training accuracy= 0.835140\n",
      "MRT: Epoch 1 : Covered 21400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.752376 : Minibatch training accuracy= 0.887196\n",
      "MRT: Epoch 1 : Covered 21420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.205030 : Minibatch training accuracy= 0.890417\n",
      "MRT: Epoch 1 : Covered 21440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.142317 : Minibatch training accuracy= 0.914736\n",
      "MRT: Epoch 1 : Covered 21460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.512344 : Minibatch training accuracy= 0.862795\n",
      "MRT: Epoch 1 : Covered 21480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.662522 : Minibatch training accuracy= 0.870841\n",
      "MRT: Epoch 1 : Covered 21500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.653632 : Minibatch training accuracy= 0.867012\n",
      "MRT: Epoch 1 : Covered 21520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.286927 : Minibatch training accuracy= 0.895674\n",
      "MRT: Epoch 1 : Covered 21540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.256083 : Minibatch training accuracy= 0.875199\n",
      "MRT: Epoch 1 : Covered 21560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.578009 : Minibatch training accuracy= 0.901718\n",
      "MRT: Epoch 1 : Covered 21580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.181472 : Minibatch training accuracy= 0.892389\n",
      "MRT: Epoch 1 : Covered 21600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.037315 : Minibatch training accuracy= 0.904009\n",
      "MRT: Epoch 1 : Covered 21620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.326034 : Minibatch training accuracy= 0.891088\n",
      "MRT: Epoch 1 : Covered 21640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.432761 : Minibatch training accuracy= 0.874373\n",
      "MRT: Epoch 1 : Covered 21660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.467107 : Minibatch training accuracy= 0.877208\n",
      "MRT: Epoch 1 : Covered 21680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.876827 : Minibatch training accuracy= 0.892460\n",
      "MRT: Epoch 1 : Covered 21700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.445283 : Minibatch training accuracy= 0.873612\n",
      "MRT: Epoch 1 : Covered 21720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.923433 : Minibatch training accuracy= 0.912234\n",
      "MRT: Epoch 1 : Covered 21740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.946744 : Minibatch training accuracy= 0.887278\n",
      "MRT: Epoch 1 : Covered 21760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.637161 : Minibatch training accuracy= 0.896197\n",
      "MRT: Epoch 1 : Covered 21780/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.099492 : Minibatch training accuracy= 0.870975\n",
      "MRT: Epoch 1 : Covered 21800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.474766 : Minibatch training accuracy= 0.907963\n",
      "MRT: Epoch 1 : Covered 21820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.219644 : Minibatch training accuracy= 0.860628\n",
      "MRT: Epoch 1 : Covered 21840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.341351 : Minibatch training accuracy= 0.902753\n",
      "MRT: Epoch 1 : Covered 21860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.207429 : Minibatch training accuracy= 0.900530\n",
      "MRT: Epoch 1 : Covered 21880/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.948478 : Minibatch training accuracy= 0.877012\n",
      "MRT: Epoch 1 : Covered 21900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.538651 : Minibatch training accuracy= 0.893763\n",
      "MRT: Epoch 1 : Covered 21920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.889691 : Minibatch training accuracy= 0.881804\n",
      "MRT: Epoch 1 : Covered 21940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.799836 : Minibatch training accuracy= 0.895157\n",
      "MRT: Epoch 1 : Covered 21960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.426503 : Minibatch training accuracy= 0.889188\n",
      "MRT: Epoch 1 : Covered 21980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.111785 : Minibatch training accuracy= 0.909203\n",
      "MRT: Epoch 1 : Covered 22000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.971217 : Minibatch training accuracy= 0.868754\n",
      "MRT: Epoch 1 : Covered 22020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.264271 : Minibatch training accuracy= 0.892151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 22040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.095759 : Minibatch training accuracy= 0.878581\n",
      "MRT: Epoch 1 : Covered 22060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.687383 : Minibatch training accuracy= 0.878971\n",
      "MRT: Epoch 1 : Covered 22080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.333657 : Minibatch training accuracy= 0.886760\n",
      "MRT: Epoch 1 : Covered 22100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.286850 : Minibatch training accuracy= 0.872322\n",
      "MRT: Epoch 1 : Covered 22120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.694437 : Minibatch training accuracy= 0.894926\n",
      "MRT: Epoch 1 : Covered 22140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.316689 : Minibatch training accuracy= 0.897143\n",
      "MRT: Epoch 1 : Covered 22160/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.779100 : Minibatch training accuracy= 0.906492\n",
      "MRT: Epoch 1 : Covered 22180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.827384 : Minibatch training accuracy= 0.878120\n",
      "MRT: Epoch 1 : Covered 22200/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.878285 : Minibatch training accuracy= 0.904970\n",
      "MRT: Epoch 1 : Covered 22220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.399178 : Minibatch training accuracy= 0.905725\n",
      "MRT: Epoch 1 : Covered 22240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.277060 : Minibatch training accuracy= 0.895097\n",
      "MRT: Epoch 1 : Covered 22260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.149871 : Minibatch training accuracy= 0.878861\n",
      "MRT: Epoch 1 : Covered 22280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.814182 : Minibatch training accuracy= 0.899822\n",
      "MRT: Epoch 1 : Covered 22300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.118731 : Minibatch training accuracy= 0.850467\n",
      "MRT: Epoch 1 : Covered 22320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.585855 : Minibatch training accuracy= 0.890255\n",
      "MRT: Epoch 1 : Covered 22340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.302280 : Minibatch training accuracy= 0.896493\n",
      "MRT: Epoch 1 : Covered 22360/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.887370 : Minibatch training accuracy= 0.878107\n",
      "MRT: Epoch 1 : Covered 22380/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.057507 : Minibatch training accuracy= 0.860451\n",
      "MRT: Epoch 1 : Covered 22400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.370500 : Minibatch training accuracy= 0.879999\n",
      "MRT: Epoch 1 : Covered 22420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.561533 : Minibatch training accuracy= 0.853704\n",
      "MRT: Epoch 1 : Covered 22440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.135970 : Minibatch training accuracy= 0.885374\n",
      "MRT: Epoch 1 : Covered 22460/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.050557 : Minibatch training accuracy= 0.883829\n",
      "MRT: Epoch 1 : Covered 22480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.419269 : Minibatch training accuracy= 0.876532\n",
      "MRT: Epoch 1 : Covered 22500/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.501194 : Minibatch training accuracy= 0.910877\n",
      "MRT: Epoch 1 : Covered 22520/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.794192 : Minibatch training accuracy= 0.892090\n",
      "MRT: Epoch 1 : Covered 22540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.173045 : Minibatch training accuracy= 0.879690\n",
      "MRT: Epoch 1 : Covered 22560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.256253 : Minibatch training accuracy= 0.876204\n",
      "MRT: Epoch 1 : Covered 22580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.667650 : Minibatch training accuracy= 0.881691\n",
      "MRT: Epoch 1 : Covered 22600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.361310 : Minibatch training accuracy= 0.863339\n",
      "MRT: Epoch 1 : Covered 22620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.611520 : Minibatch training accuracy= 0.861091\n",
      "MRT: Epoch 1 : Covered 22640/90262 : Minibatch Reward Weighted Multisample CE Loss= 6.092915 : Minibatch training accuracy= 0.538574\n",
      "MRT: Epoch 1 : Covered 22660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.410269 : Minibatch training accuracy= 0.908536\n",
      "MRT: Epoch 1 : Covered 22680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.110399 : Minibatch training accuracy= 0.881101\n",
      "MRT: Epoch 1 : Covered 22700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.311272 : Minibatch training accuracy= 0.860858\n",
      "MRT: Epoch 1 : Covered 22720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.261862 : Minibatch training accuracy= 0.919205\n",
      "MRT: Epoch 1 : Covered 22740/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.767810 : Minibatch training accuracy= 0.824489\n",
      "MRT: Epoch 1 : Covered 22760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.272195 : Minibatch training accuracy= 0.872241\n",
      "MRT: Epoch 1 : Covered 22780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.061736 : Minibatch training accuracy= 0.890489\n",
      "MRT: Epoch 1 : Covered 22800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.638313 : Minibatch training accuracy= 0.888520\n",
      "MRT: Epoch 1 : Covered 22820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.648409 : Minibatch training accuracy= 0.892004\n",
      "MRT: Epoch 1 : Covered 22840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.166710 : Minibatch training accuracy= 0.883126\n",
      "MRT: Epoch 1 : Covered 22860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.473142 : Minibatch training accuracy= 0.881266\n",
      "MRT: Epoch 1 : Covered 22880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.077359 : Minibatch training accuracy= 0.896013\n",
      "MRT: Epoch 1 : Covered 22900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.048541 : Minibatch training accuracy= 0.897475\n",
      "MRT: Epoch 1 : Covered 22920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.855117 : Minibatch training accuracy= 0.893344\n",
      "MRT: Epoch 1 : Covered 22940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.103549 : Minibatch training accuracy= 0.891220\n",
      "MRT: Epoch 1 : Covered 22960/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.977825 : Minibatch training accuracy= 0.897550\n",
      "MRT: Epoch 1 : Covered 22980/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.795972 : Minibatch training accuracy= 0.884969\n",
      "MRT: Epoch 1 : Covered 23000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.129643 : Minibatch training accuracy= 0.899162\n",
      "MRT: Epoch 1 : Covered 23020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.767928 : Minibatch training accuracy= 0.866448\n",
      "MRT: Epoch 1 : Covered 23040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.411663 : Minibatch training accuracy= 0.907785\n",
      "MRT: Epoch 1 : Covered 23060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.133089 : Minibatch training accuracy= 0.859162\n",
      "MRT: Epoch 1 : Covered 23080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.764347 : Minibatch training accuracy= 0.909760\n",
      "MRT: Epoch 1 : Covered 23100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.345283 : Minibatch training accuracy= 0.897446\n",
      "MRT: Epoch 1 : Covered 23120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.845412 : Minibatch training accuracy= 0.915840\n",
      "MRT: Epoch 1 : Covered 23140/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.150313 : Minibatch training accuracy= 0.916159\n",
      "MRT: Epoch 1 : Covered 23160/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.976737 : Minibatch training accuracy= 0.860970\n",
      "MRT: Epoch 1 : Covered 23180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.283321 : Minibatch training accuracy= 0.903761\n",
      "MRT: Epoch 1 : Covered 23200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.161813 : Minibatch training accuracy= 0.866682\n",
      "MRT: Epoch 1 : Covered 23220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.869106 : Minibatch training accuracy= 0.896831\n",
      "MRT: Epoch 1 : Covered 23240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.375484 : Minibatch training accuracy= 0.852667\n",
      "MRT: Epoch 1 : Covered 23260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.559716 : Minibatch training accuracy= 0.877307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 23280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.185078 : Minibatch training accuracy= 0.900075\n",
      "MRT: Epoch 1 : Covered 23300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.757254 : Minibatch training accuracy= 0.899100\n",
      "MRT: Epoch 1 : Covered 23320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.844405 : Minibatch training accuracy= 0.887102\n",
      "MRT: Epoch 1 : Covered 23340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.175537 : Minibatch training accuracy= 0.907000\n",
      "MRT: Epoch 1 : Covered 23360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.289738 : Minibatch training accuracy= 0.894154\n",
      "MRT: Epoch 1 : Covered 23380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.075922 : Minibatch training accuracy= 0.906555\n",
      "MRT: Epoch 1 : Covered 23400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.732002 : Minibatch training accuracy= 0.936483\n",
      "MRT: Epoch 1 : Covered 23420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.004507 : Minibatch training accuracy= 0.889075\n",
      "MRT: Epoch 1 : Covered 23440/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.731456 : Minibatch training accuracy= 0.905738\n",
      "MRT: Epoch 1 : Covered 23460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.127698 : Minibatch training accuracy= 0.886430\n",
      "MRT: Epoch 1 : Covered 23480/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.905746 : Minibatch training accuracy= 0.873998\n",
      "MRT: Epoch 1 : Covered 23500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.130738 : Minibatch training accuracy= 0.894601\n",
      "MRT: Epoch 1 : Covered 23520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.145914 : Minibatch training accuracy= 0.890450\n",
      "MRT: Epoch 1 : Covered 23540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.437229 : Minibatch training accuracy= 0.870901\n",
      "MRT: Epoch 1 : Covered 23560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.269336 : Minibatch training accuracy= 0.894008\n",
      "MRT: Epoch 1 : Covered 23580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.514763 : Minibatch training accuracy= 0.880305\n",
      "MRT: Epoch 1 : Covered 23600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.333959 : Minibatch training accuracy= 0.870662\n",
      "MRT: Epoch 1 : Covered 23620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.352309 : Minibatch training accuracy= 0.884398\n",
      "MRT: Epoch 1 : Covered 23640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.431950 : Minibatch training accuracy= 0.855065\n",
      "MRT: Epoch 1 : Covered 23660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.875302 : Minibatch training accuracy= 0.902370\n",
      "MRT: Epoch 1 : Covered 23680/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.099719 : Minibatch training accuracy= 0.914590\n",
      "MRT: Epoch 1 : Covered 23700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.938328 : Minibatch training accuracy= 0.864290\n",
      "MRT: Epoch 1 : Covered 23720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.783784 : Minibatch training accuracy= 0.895029\n",
      "MRT: Epoch 1 : Covered 23740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.930085 : Minibatch training accuracy= 0.928326\n",
      "MRT: Epoch 1 : Covered 23760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.439985 : Minibatch training accuracy= 0.916994\n",
      "MRT: Epoch 1 : Covered 23780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.160898 : Minibatch training accuracy= 0.915024\n",
      "MRT: Epoch 1 : Covered 23800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.565365 : Minibatch training accuracy= 0.895027\n",
      "MRT: Epoch 1 : Covered 23820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.020841 : Minibatch training accuracy= 0.884186\n",
      "MRT: Epoch 1 : Covered 23840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.213763 : Minibatch training accuracy= 0.870720\n",
      "MRT: Epoch 1 : Covered 23860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.783618 : Minibatch training accuracy= 0.865861\n",
      "MRT: Epoch 1 : Covered 23880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.385206 : Minibatch training accuracy= 0.869437\n",
      "MRT: Epoch 1 : Covered 23900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.194454 : Minibatch training accuracy= 0.876323\n",
      "MRT: Epoch 1 : Covered 23920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.214011 : Minibatch training accuracy= 0.896629\n",
      "MRT: Epoch 1 : Covered 23940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.584902 : Minibatch training accuracy= 0.884554\n",
      "MRT: Epoch 1 : Covered 23960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.128300 : Minibatch training accuracy= 0.895271\n",
      "MRT: Epoch 1 : Covered 23980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.000302 : Minibatch training accuracy= 0.904228\n",
      "MRT: Epoch 1 : Covered 24000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.310961 : Minibatch training accuracy= 0.897484\n",
      "MRT: Epoch 1 : Covered 24020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.088165 : Minibatch training accuracy= 0.902189\n",
      "MRT: Epoch 1 : Covered 24040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.375462 : Minibatch training accuracy= 0.893926\n",
      "MRT: Epoch 1 : Covered 24060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.781115 : Minibatch training accuracy= 0.877129\n",
      "MRT: Epoch 1 : Covered 24080/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.999874 : Minibatch training accuracy= 0.888945\n",
      "MRT: Epoch 1 : Covered 24100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.488376 : Minibatch training accuracy= 0.866457\n",
      "MRT: Epoch 1 : Covered 24120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.544941 : Minibatch training accuracy= 0.917049\n",
      "MRT: Epoch 1 : Covered 24140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.504040 : Minibatch training accuracy= 0.909152\n",
      "MRT: Epoch 1 : Covered 24160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.878227 : Minibatch training accuracy= 0.903615\n",
      "MRT: Epoch 1 : Covered 24180/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.824689 : Minibatch training accuracy= 0.885183\n",
      "MRT: Epoch 1 : Covered 24200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.401047 : Minibatch training accuracy= 0.875548\n",
      "MRT: Epoch 1 : Covered 24220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.641619 : Minibatch training accuracy= 0.880189\n",
      "MRT: Epoch 1 : Covered 24240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.193461 : Minibatch training accuracy= 0.896739\n",
      "MRT: Epoch 1 : Covered 24260/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.838992 : Minibatch training accuracy= 0.907208\n",
      "MRT: Epoch 1 : Covered 24280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.151046 : Minibatch training accuracy= 0.892330\n",
      "MRT: Epoch 1 : Covered 24300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.695299 : Minibatch training accuracy= 0.883155\n",
      "MRT: Epoch 1 : Covered 24320/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.908524 : Minibatch training accuracy= 0.904653\n",
      "MRT: Epoch 1 : Covered 24340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.214406 : Minibatch training accuracy= 0.893575\n",
      "MRT: Epoch 1 : Covered 24360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.152531 : Minibatch training accuracy= 0.889384\n",
      "MRT: Epoch 1 : Covered 24380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.249618 : Minibatch training accuracy= 0.879563\n",
      "MRT: Epoch 1 : Covered 24400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.579104 : Minibatch training accuracy= 0.908904\n",
      "MRT: Epoch 1 : Covered 24420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.730191 : Minibatch training accuracy= 0.900262\n",
      "MRT: Epoch 1 : Covered 24440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.465223 : Minibatch training accuracy= 0.888675\n",
      "MRT: Epoch 1 : Covered 24460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.212023 : Minibatch training accuracy= 0.886106\n",
      "MRT: Epoch 1 : Covered 24480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.750577 : Minibatch training accuracy= 0.882036\n",
      "MRT: Epoch 1 : Covered 24500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.131933 : Minibatch training accuracy= 0.902284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 24520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.066102 : Minibatch training accuracy= 0.888594\n",
      "MRT: Epoch 1 : Covered 24540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.228658 : Minibatch training accuracy= 0.873832\n",
      "MRT: Epoch 1 : Covered 24560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.266701 : Minibatch training accuracy= 0.909087\n",
      "MRT: Epoch 1 : Covered 24580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.147434 : Minibatch training accuracy= 0.912129\n",
      "MRT: Epoch 1 : Covered 24600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.723759 : Minibatch training accuracy= 0.904617\n",
      "MRT: Epoch 1 : Covered 24620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.130467 : Minibatch training accuracy= 0.870530\n",
      "MRT: Epoch 1 : Covered 24640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.194736 : Minibatch training accuracy= 0.918144\n",
      "MRT: Epoch 1 : Covered 24660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.309398 : Minibatch training accuracy= 0.880625\n",
      "MRT: Epoch 1 : Covered 24680/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.799182 : Minibatch training accuracy= 0.877703\n",
      "MRT: Epoch 1 : Covered 24700/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.325017 : Minibatch training accuracy= 0.882265\n",
      "MRT: Epoch 1 : Covered 24720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.660002 : Minibatch training accuracy= 0.884913\n",
      "MRT: Epoch 1 : Covered 24740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.425000 : Minibatch training accuracy= 0.892858\n",
      "MRT: Epoch 1 : Covered 24760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.234501 : Minibatch training accuracy= 0.898990\n",
      "MRT: Epoch 1 : Covered 24780/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.683695 : Minibatch training accuracy= 0.908361\n",
      "MRT: Epoch 1 : Covered 24800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.061368 : Minibatch training accuracy= 0.893868\n",
      "MRT: Epoch 1 : Covered 24820/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.956135 : Minibatch training accuracy= 0.881854\n",
      "MRT: Epoch 1 : Covered 24840/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.202616 : Minibatch training accuracy= 0.896541\n",
      "MRT: Epoch 1 : Covered 24860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.186993 : Minibatch training accuracy= 0.867259\n",
      "MRT: Epoch 1 : Covered 24880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.468424 : Minibatch training accuracy= 0.879352\n",
      "MRT: Epoch 1 : Covered 24900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.461957 : Minibatch training accuracy= 0.872903\n",
      "MRT: Epoch 1 : Covered 24920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.163629 : Minibatch training accuracy= 0.908289\n",
      "MRT: Epoch 1 : Covered 24940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.237939 : Minibatch training accuracy= 0.885624\n",
      "MRT: Epoch 1 : Covered 24960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.124173 : Minibatch training accuracy= 0.876159\n",
      "MRT: Epoch 1 : Covered 24980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.254933 : Minibatch training accuracy= 0.904704\n",
      "MRT: Epoch 1 : Covered 25000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.370120 : Minibatch training accuracy= 0.907187\n",
      "MRT: Epoch 1 : Covered 25020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.130363 : Minibatch training accuracy= 0.869627\n",
      "MRT: Epoch 1 : Covered 25040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.304691 : Minibatch training accuracy= 0.884358\n",
      "MRT: Epoch 1 : Covered 25060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.306361 : Minibatch training accuracy= 0.895410\n",
      "MRT: Epoch 1 : Covered 25080/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.115714 : Minibatch training accuracy= 0.902143\n",
      "MRT: Epoch 1 : Covered 25100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.197664 : Minibatch training accuracy= 0.906996\n",
      "MRT: Epoch 1 : Covered 25120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.035623 : Minibatch training accuracy= 0.881554\n",
      "MRT: Epoch 1 : Covered 25140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.320166 : Minibatch training accuracy= 0.888560\n",
      "MRT: Epoch 1 : Covered 25160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.416085 : Minibatch training accuracy= 0.885722\n",
      "MRT: Epoch 1 : Covered 25180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.483984 : Minibatch training accuracy= 0.915885\n",
      "MRT: Epoch 1 : Covered 25200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.447942 : Minibatch training accuracy= 0.900513\n",
      "MRT: Epoch 1 : Covered 25220/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.910203 : Minibatch training accuracy= 0.909738\n",
      "MRT: Epoch 1 : Covered 25240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.318864 : Minibatch training accuracy= 0.887491\n",
      "MRT: Epoch 1 : Covered 25260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.262000 : Minibatch training accuracy= 0.874715\n",
      "MRT: Epoch 1 : Covered 25280/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.962706 : Minibatch training accuracy= 0.845232\n",
      "MRT: Epoch 1 : Covered 25300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.966121 : Minibatch training accuracy= 0.900484\n",
      "MRT: Epoch 1 : Covered 25320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.877663 : Minibatch training accuracy= 0.904679\n",
      "MRT: Epoch 1 : Covered 25340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.414399 : Minibatch training accuracy= 0.910821\n",
      "MRT: Epoch 1 : Covered 25360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.143747 : Minibatch training accuracy= 0.874936\n",
      "MRT: Epoch 1 : Covered 25380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.174072 : Minibatch training accuracy= 0.885677\n",
      "MRT: Epoch 1 : Covered 25400/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.967565 : Minibatch training accuracy= 0.887806\n",
      "MRT: Epoch 1 : Covered 25420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.331170 : Minibatch training accuracy= 0.882154\n",
      "MRT: Epoch 1 : Covered 25440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.121709 : Minibatch training accuracy= 0.906509\n",
      "MRT: Epoch 1 : Covered 25460/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.966800 : Minibatch training accuracy= 0.863835\n",
      "MRT: Epoch 1 : Covered 25480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.485533 : Minibatch training accuracy= 0.919067\n",
      "MRT: Epoch 1 : Covered 25500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.038238 : Minibatch training accuracy= 0.896036\n",
      "MRT: Epoch 1 : Covered 25520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.221534 : Minibatch training accuracy= 0.895838\n",
      "MRT: Epoch 1 : Covered 25540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.316240 : Minibatch training accuracy= 0.896605\n",
      "MRT: Epoch 1 : Covered 25560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.363302 : Minibatch training accuracy= 0.906221\n",
      "MRT: Epoch 1 : Covered 25580/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.943709 : Minibatch training accuracy= 0.847394\n",
      "MRT: Epoch 1 : Covered 25600/90262 : Minibatch Reward Weighted Multisample CE Loss= 5.608882 : Minibatch training accuracy= 0.885204\n",
      "MRT: Epoch 1 : Covered 25620/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.037207 : Minibatch training accuracy= 0.908837\n",
      "MRT: Epoch 1 : Covered 25640/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.678977 : Minibatch training accuracy= 0.874192\n",
      "MRT: Epoch 1 : Covered 25660/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.555622 : Minibatch training accuracy= 0.862223\n",
      "MRT: Epoch 1 : Covered 25680/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.541601 : Minibatch training accuracy= 0.854092\n",
      "MRT: Epoch 1 : Covered 25700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.611340 : Minibatch training accuracy= 0.864486\n",
      "MRT: Epoch 1 : Covered 25720/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.798592 : Minibatch training accuracy= 0.809272\n",
      "MRT: Epoch 1 : Covered 25740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.231981 : Minibatch training accuracy= 0.852914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 25760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.019235 : Minibatch training accuracy= 0.872383\n",
      "MRT: Epoch 1 : Covered 25780/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.969896 : Minibatch training accuracy= 0.885509\n",
      "MRT: Epoch 1 : Covered 25800/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.871511 : Minibatch training accuracy= 0.910596\n",
      "MRT: Epoch 1 : Covered 25820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.138437 : Minibatch training accuracy= 0.871288\n",
      "MRT: Epoch 1 : Covered 25840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.924735 : Minibatch training accuracy= 0.865510\n",
      "MRT: Epoch 1 : Covered 25860/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.555611 : Minibatch training accuracy= 0.892417\n",
      "MRT: Epoch 1 : Covered 25880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.506713 : Minibatch training accuracy= 0.913986\n",
      "MRT: Epoch 1 : Covered 25900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.264918 : Minibatch training accuracy= 0.902331\n",
      "MRT: Epoch 1 : Covered 25920/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.455201 : Minibatch training accuracy= 0.918753\n",
      "MRT: Epoch 1 : Covered 25940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.634544 : Minibatch training accuracy= 0.868069\n",
      "MRT: Epoch 1 : Covered 25960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.551531 : Minibatch training accuracy= 0.956962\n",
      "MRT: Epoch 1 : Covered 25980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.275095 : Minibatch training accuracy= 0.864656\n",
      "MRT: Epoch 1 : Covered 26000/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.013486 : Minibatch training accuracy= 0.916286\n",
      "MRT: Epoch 1 : Covered 26020/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.389287 : Minibatch training accuracy= 0.911243\n",
      "MRT: Epoch 1 : Covered 26040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.451588 : Minibatch training accuracy= 0.902552\n",
      "MRT: Epoch 1 : Covered 26060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.431058 : Minibatch training accuracy= 0.907720\n",
      "MRT: Epoch 1 : Covered 26080/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.858276 : Minibatch training accuracy= 0.899864\n",
      "MRT: Epoch 1 : Covered 26100/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.373895 : Minibatch training accuracy= 0.912184\n",
      "MRT: Epoch 1 : Covered 26120/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.961335 : Minibatch training accuracy= 0.887465\n",
      "MRT: Epoch 1 : Covered 26140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.259010 : Minibatch training accuracy= 0.908458\n",
      "MRT: Epoch 1 : Covered 26160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.353724 : Minibatch training accuracy= 0.895213\n",
      "MRT: Epoch 1 : Covered 26180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.602718 : Minibatch training accuracy= 0.867978\n",
      "MRT: Epoch 1 : Covered 26200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.616977 : Minibatch training accuracy= 0.885999\n",
      "MRT: Epoch 1 : Covered 26220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.299782 : Minibatch training accuracy= 0.878725\n",
      "MRT: Epoch 1 : Covered 26240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.354167 : Minibatch training accuracy= 0.906448\n",
      "MRT: Epoch 1 : Covered 26260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.575408 : Minibatch training accuracy= 0.897190\n",
      "MRT: Epoch 1 : Covered 26280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.227875 : Minibatch training accuracy= 0.908930\n",
      "MRT: Epoch 1 : Covered 26300/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.817856 : Minibatch training accuracy= 0.908132\n",
      "MRT: Epoch 1 : Covered 26320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.550430 : Minibatch training accuracy= 0.901179\n",
      "MRT: Epoch 1 : Covered 26340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.476391 : Minibatch training accuracy= 0.937193\n",
      "MRT: Epoch 1 : Covered 26360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.909684 : Minibatch training accuracy= 0.909012\n",
      "MRT: Epoch 1 : Covered 26380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.250310 : Minibatch training accuracy= 0.893197\n",
      "MRT: Epoch 1 : Covered 26400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.281525 : Minibatch training accuracy= 0.868462\n",
      "MRT: Epoch 1 : Covered 26420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.326015 : Minibatch training accuracy= 0.908015\n",
      "MRT: Epoch 1 : Covered 26440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.690877 : Minibatch training accuracy= 0.867659\n",
      "MRT: Epoch 1 : Covered 26460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.590998 : Minibatch training accuracy= 0.865859\n",
      "MRT: Epoch 1 : Covered 26480/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.115483 : Minibatch training accuracy= 0.894054\n",
      "MRT: Epoch 1 : Covered 26500/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.377519 : Minibatch training accuracy= 0.894744\n",
      "MRT: Epoch 1 : Covered 26520/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.252347 : Minibatch training accuracy= 0.918260\n",
      "MRT: Epoch 1 : Covered 26540/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.149319 : Minibatch training accuracy= 0.908884\n",
      "MRT: Epoch 1 : Covered 26560/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.174868 : Minibatch training accuracy= 0.881626\n",
      "MRT: Epoch 1 : Covered 26580/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.304136 : Minibatch training accuracy= 0.879961\n",
      "MRT: Epoch 1 : Covered 26600/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.238644 : Minibatch training accuracy= 0.883010\n",
      "MRT: Epoch 1 : Covered 26620/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.975805 : Minibatch training accuracy= 0.873749\n",
      "MRT: Epoch 1 : Covered 26640/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.273684 : Minibatch training accuracy= 0.848388\n",
      "MRT: Epoch 1 : Covered 26660/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.286716 : Minibatch training accuracy= 0.903219\n",
      "MRT: Epoch 1 : Covered 26680/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.646004 : Minibatch training accuracy= 0.919113\n",
      "MRT: Epoch 1 : Covered 26700/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.117503 : Minibatch training accuracy= 0.901955\n",
      "MRT: Epoch 1 : Covered 26720/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.187034 : Minibatch training accuracy= 0.921558\n",
      "MRT: Epoch 1 : Covered 26740/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.517577 : Minibatch training accuracy= 0.902933\n",
      "MRT: Epoch 1 : Covered 26760/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.024699 : Minibatch training accuracy= 0.920125\n",
      "MRT: Epoch 1 : Covered 26780/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.977715 : Minibatch training accuracy= 0.879621\n",
      "MRT: Epoch 1 : Covered 26800/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.170490 : Minibatch training accuracy= 0.906831\n",
      "MRT: Epoch 1 : Covered 26820/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.436063 : Minibatch training accuracy= 0.904683\n",
      "MRT: Epoch 1 : Covered 26840/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.754930 : Minibatch training accuracy= 0.867685\n",
      "MRT: Epoch 1 : Covered 26860/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.954284 : Minibatch training accuracy= 0.904747\n",
      "MRT: Epoch 1 : Covered 26880/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.398059 : Minibatch training accuracy= 0.885303\n",
      "MRT: Epoch 1 : Covered 26900/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.148737 : Minibatch training accuracy= 0.855694\n",
      "MRT: Epoch 1 : Covered 26920/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.970828 : Minibatch training accuracy= 0.904747\n",
      "MRT: Epoch 1 : Covered 26940/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.545022 : Minibatch training accuracy= 0.874025\n",
      "MRT: Epoch 1 : Covered 26960/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.653604 : Minibatch training accuracy= 0.902817\n",
      "MRT: Epoch 1 : Covered 26980/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.431802 : Minibatch training accuracy= 0.926641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRT: Epoch 1 : Covered 27000/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.864309 : Minibatch training accuracy= 0.872321\n",
      "MRT: Epoch 1 : Covered 27020/90262 : Minibatch Reward Weighted Multisample CE Loss= 2.201738 : Minibatch training accuracy= 0.892941\n",
      "MRT: Epoch 1 : Covered 27040/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.228318 : Minibatch training accuracy= 0.886678\n",
      "MRT: Epoch 1 : Covered 27060/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.893368 : Minibatch training accuracy= 0.873170\n",
      "MRT: Epoch 1 : Covered 27080/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.320212 : Minibatch training accuracy= 0.871565\n",
      "MRT: Epoch 1 : Covered 27100/90262 : Minibatch Reward Weighted Multisample CE Loss= 4.163011 : Minibatch training accuracy= 0.863689\n",
      "MRT: Epoch 1 : Covered 27120/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.870142 : Minibatch training accuracy= 0.889648\n",
      "MRT: Epoch 1 : Covered 27140/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.563499 : Minibatch training accuracy= 0.891448\n",
      "MRT: Epoch 1 : Covered 27160/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.402804 : Minibatch training accuracy= 0.863413\n",
      "MRT: Epoch 1 : Covered 27180/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.196714 : Minibatch training accuracy= 0.884076\n",
      "MRT: Epoch 1 : Covered 27200/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.363453 : Minibatch training accuracy= 0.915157\n",
      "MRT: Epoch 1 : Covered 27220/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.326155 : Minibatch training accuracy= 0.896175\n",
      "MRT: Epoch 1 : Covered 27240/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.317696 : Minibatch training accuracy= 0.858477\n",
      "MRT: Epoch 1 : Covered 27260/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.636962 : Minibatch training accuracy= 0.903972\n",
      "MRT: Epoch 1 : Covered 27280/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.210181 : Minibatch training accuracy= 0.876604\n",
      "MRT: Epoch 1 : Covered 27300/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.134264 : Minibatch training accuracy= 0.910419\n",
      "MRT: Epoch 1 : Covered 27320/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.762323 : Minibatch training accuracy= 0.899316\n",
      "MRT: Epoch 1 : Covered 27340/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.355238 : Minibatch training accuracy= 0.872709\n",
      "MRT: Epoch 1 : Covered 27360/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.239602 : Minibatch training accuracy= 0.854517\n",
      "MRT: Epoch 1 : Covered 27380/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.466558 : Minibatch training accuracy= 0.879864\n",
      "MRT: Epoch 1 : Covered 27400/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.546772 : Minibatch training accuracy= 0.897325\n",
      "MRT: Epoch 1 : Covered 27420/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.231977 : Minibatch training accuracy= 0.832993\n",
      "MRT: Epoch 1 : Covered 27440/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.267125 : Minibatch training accuracy= 0.919115\n",
      "MRT: Epoch 1 : Covered 27460/90262 : Minibatch Reward Weighted Multisample CE Loss= 3.495560 : Minibatch training accuracy= 0.897657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bbd4cffb3c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bbd4cffb3c4d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bbd4cffb3c4d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m                                                                              \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactual_reward_multisample_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_reward_multiple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                                                                              \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                                                                              model.weight_placeholder: batch_weight})\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m# Print Summary to Tensor Board\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################## Batch Testing a model on some dataset ############\n",
    "\n",
    "def batch_predict_with_a_model(data, model, session=None):\n",
    "\n",
    "  data_logits = []\n",
    "  data_labels = []\n",
    "  data_weights = []\n",
    "  \n",
    "  step = 1\n",
    "  while (step * FLAGS.batch_size) <= len(data.fileindices):\n",
    "    # Get batch data as Numpy Arrays : Without shuffling\n",
    "    batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = data.get_batch(((step-1)*FLAGS.batch_size), (step * FLAGS.batch_size))\n",
    "    batch_logits = session.run(model.logits, feed_dict={model.document_placeholder: batch_docs})\n",
    "    \n",
    "    data_logits.append(batch_logits)\n",
    "    data_labels.append(batch_label)\n",
    "    data_weights.append(batch_weight)\n",
    "    \n",
    "    # Increase step\n",
    "    step += 1\n",
    "\n",
    "  # Check if any data left\n",
    "  if (len(data.fileindices) > ((step-1)*FLAGS.batch_size)):\n",
    "    # Get last batch as Numpy Arrays\n",
    "    batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = data.get_batch(((step-1)*FLAGS.batch_size), len(data.fileindices))\n",
    "    batch_logits = session.run(model.logits, feed_dict={model.document_placeholder: batch_docs})\n",
    "    \n",
    "    data_logits.append(batch_logits)\n",
    "    data_labels.append(batch_label)\n",
    "    data_weights.append(batch_weight)\n",
    "    # print(data_logits) \n",
    "    \n",
    "  # Convert list to tensors\n",
    "  data_logits = tf.concat(0, data_logits)\n",
    "  data_lables = tf.concat(0, data_labels)\n",
    "  data_weights = tf.concat(0, data_weights)\n",
    "  # print(data_logits,data_lables,data_weights)\n",
    "  return data_logits, data_lables, data_weights \n",
    "\n",
    "######################## Training Mode ###########################\n",
    "\n",
    "def train():\n",
    "  \"\"\"\n",
    "  Training Mode: Create a new model and train the network\n",
    "  \"\"\"\n",
    "\n",
    "  # Training: use the tf default graph\n",
    "  with tf.Graph().as_default() and tf.device(FLAGS.use_gpu):\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    \n",
    "    # Start a session\n",
    "    with tf.Session(config = config) as sess:\n",
    "      \n",
    "      ### Prepare data for training\n",
    "      print(\"Prepare vocab dict and read pretrained word embeddings ...\")\n",
    "      vocab_dict, word_embedding_array = DataProcessor().prepare_vocab_embeddingdict()\n",
    "      # vocab_dict contains _PAD and _UNK but not word_embedding_array\n",
    "\n",
    "      print(\"Prepare training data ...\")\n",
    "      train_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"training\")\n",
    "      \n",
    "      print(\"Prepare validation data ...\")\n",
    "      validation_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"validation\")\n",
    "\n",
    "      print(\"Prepare ROUGE reward generator ...\")\n",
    "      rouge_generator = Reward_Generator()\n",
    "\n",
    "      # Create Model with various operations\n",
    "      model = MY_Model(sess, len(vocab_dict)-2)\n",
    "      \n",
    "      # Start training with some pretrained model\n",
    "      start_epoch = 1\n",
    "      # selected_modelpath = FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(start_epoch-1)\n",
    "      # if not (os.path.isfile(selected_modelpath)):\n",
    "      #   print(\"Model not found in checkpoint folder.\")\n",
    "      #   exit(0)\n",
    "      # # Reload saved model and test\n",
    "      # print(\"Reading model parameters from %s\" % selected_modelpath)\n",
    "      # model.saver.restore(sess, selected_modelpath)\n",
    "      # print(\"Model loaded.\")\n",
    "\n",
    "      # Initialize word embedding before training\n",
    "      print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "      sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "      ########### Start (No Mixer) Training : Reinforcement learning ################\n",
    "      # Reward aware training as part of Reward weighted CE ,  \n",
    "      # No Curriculam learning: No annealing, consider full document like in MRT\n",
    "      # Multiple Samples (include gold sample),  No future reward, Similar to MRT\n",
    "      # During training does not use PYROUGE to avoid multiple file rewritings\n",
    "      # Approximate MRT with multiple pre-estimated oracle samples\n",
    "      # June 2017: Use Single sample from multiple oracles\n",
    "      ###############################################################################\n",
    "\n",
    "      print(\"Start Reinforcement Training (single rollout from largest prob mass) ...\")\n",
    "\n",
    "      for epoch in range(start_epoch, FLAGS.train_epoch_wce + 1):\n",
    "        print(\"MRT: Epoch \"+str(epoch))\n",
    "        \n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Reshuffle training document indices\")\n",
    "        train_data.shuffle_fileindices()\n",
    "          \n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Restore Rouge Dict\")\n",
    "        rouge_generator.restore_rouge_dict()\n",
    "          \n",
    "        # Start Batch Training\n",
    "        step = 1\n",
    "        while (step * FLAGS.batch_size) <= len(train_data.fileindices):\n",
    "          # Get batch data as Numpy Arrays\n",
    "          batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = train_data.get_batch(((step-1)*FLAGS.batch_size), \n",
    "                                                                                                                                     (step * FLAGS.batch_size))\n",
    "          # print(batch_docnames)\n",
    "          # print(batch_label[0])\n",
    "          # print(batch_weight[0])\n",
    "          # print(batch_oracle_multiple[0])\n",
    "          # print(batch_reward_multiple[0])\n",
    "          # exit(0)\n",
    "          \n",
    "          # Print the progress \n",
    "          if (step % FLAGS.training_checkpoint)  == 0:\n",
    "            \n",
    "            ce_loss_val, ce_loss_sum, acc_val, acc_sum = sess.run([model.rewardweighted_cross_entropy_loss_multisample, model.rewardweighted_ce_multisample_loss_summary, \n",
    "                                                                   model.accuracy, model.taccuracy_summary],\n",
    "                                                                  feed_dict={model.document_placeholder: batch_docs, \n",
    "                                                                             model.predicted_multisample_label_placeholder: batch_oracle_multiple, \n",
    "                                                                             model.actual_reward_multisample_placeholder: batch_reward_multiple,\n",
    "                                                                             model.label_placeholder: batch_label,\n",
    "                                                                             model.weight_placeholder: batch_weight})\n",
    "            \n",
    "            # Print Summary to Tensor Board\n",
    "            model.summary_writer.add_summary(ce_loss_sum, ((epoch-1)*len(train_data.fileindices)+ step*FLAGS.batch_size))\n",
    "            model.summary_writer.add_summary(acc_sum, ((epoch-1)*len(train_data.fileindices)+step*FLAGS.batch_size))\n",
    "\n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Covered \" + str(step*FLAGS.batch_size)+\"/\"+str(len(train_data.fileindices)) + \n",
    "                  \" : Minibatch Reward Weighted Multisample CE Loss= {:.6f}\".format(ce_loss_val) + \" : Minibatch training accuracy= {:.6f}\".format(acc_val))\n",
    "\n",
    "          # Run optimizer: optimize policy network \n",
    "          sess.run([model.train_op_policynet_expreward], feed_dict={model.document_placeholder: batch_docs, \n",
    "                                                                    model.predicted_multisample_label_placeholder: batch_oracle_multiple, \n",
    "                                                                    model.actual_reward_multisample_placeholder: batch_reward_multiple,\n",
    "                                                                    model.weight_placeholder: batch_weight})\n",
    "          \n",
    "          # Increase step\n",
    "          step += 1\n",
    "          \n",
    "          # if step == 20:\n",
    "          #   break \n",
    "\n",
    "        # Save Model \n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Saving model after epoch completion\")\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"model.ckpt.epoch-\"+str(epoch))\n",
    "        model.saver.save(sess, checkpoint_path)\n",
    "        \n",
    "        # Backup Rouge Dict\n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Saving rouge dictionary\")\n",
    "        rouge_generator.save_rouge_dict()\n",
    "        \n",
    "        # Performance on the validation set \n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Performance on the validation data\")\n",
    "        # Get Predictions: Prohibit the use of gold labels\n",
    "        validation_logits, validation_labels, validation_weights = batch_predict_with_a_model(validation_data, model, session=sess)\n",
    "        # Validation Accuracy and Prediction\n",
    "        validation_acc, validation_sum = sess.run([model.final_accuracy, model.vaccuracy_summary], feed_dict={model.logits_placeholder: validation_logits.eval(session=sess), \n",
    "                                                                                                              model.label_placeholder: validation_labels.eval(session=sess), \n",
    "                                                                                                              model.weight_placeholder: validation_weights.eval(session=sess)})\n",
    "        # Print Validation Summary \n",
    "        model.summary_writer.add_summary(validation_sum, (epoch*len(train_data.fileindices)))\n",
    "        \n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Validation (\"+str(len(validation_data.fileindices))+\") accuracy= {:.6f}\".format(validation_acc))\n",
    "        # Writing validation predictions and final summaries\n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Writing final validation summaries\")\n",
    "        validation_data.write_prediction_summaries(validation_logits, \"model.ckpt.epoch-\"+str(epoch), session=sess)\n",
    "        # Extimate Rouge Scores\n",
    "        rouge_score = rouge_generator.get_full_rouge(FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(epoch)+\".validation-summary-topranked\", \"validation\")\n",
    "        print(\"MRT: Epoch \"+str(epoch)+\" : Validation (\"+str(len(validation_data.fileindices))+\") rouge= {:.6f}\".format(rouge_score))\n",
    "        \n",
    "        # break\n",
    "\n",
    "      print(\"Optimization Finished!\")\n",
    "\n",
    "# ######################## Test Mode ###########################\n",
    "\n",
    "def test():\n",
    "  \"\"\"\n",
    "  Test Mode: Loads an existing model and test it on the test set\n",
    "  \"\"\"\n",
    "  \n",
    "  # Training: use the tf default graph\n",
    "\n",
    "  with tf.Graph().as_default() and tf.device(FLAGS.use_gpu):\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "    # Start a session\n",
    "    with tf.Session(config = config) as sess:\n",
    "           \n",
    "      ### Prepare data for training\n",
    "      print(\"Prepare vocab dict and read pretrained word embeddings ...\")\n",
    "      vocab_dict, word_embedding_array = DataProcessor().prepare_vocab_embeddingdict()\n",
    "      # vocab_dict contains _PAD and _UNK but not word_embedding_array\n",
    "      \n",
    "      print(\"Prepare test data ...\")\n",
    "      test_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"test\")\n",
    "\n",
    "      # Create Model with various operations\n",
    "      model = MY_Model(sess, len(vocab_dict)-2)\n",
    "\n",
    "      # # Initialize word embedding before training\n",
    "      # print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "      # sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "      # Select the model\n",
    "      if (os.path.isfile(FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(FLAGS.model_to_load))):\n",
    "        selected_modelpath = FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(FLAGS.model_to_load)\n",
    "      else:\n",
    "        print(\"Model not found in checkpoint folder.\")\n",
    "        exit(0)\n",
    "      \n",
    "      # Reload saved model and test\n",
    "      print(\"Reading model parameters from %s\" % selected_modelpath)\n",
    "      model.saver.restore(sess, selected_modelpath)\n",
    "      print(\"Model loaded.\")\n",
    "\n",
    "      # Initialize word embedding before training\n",
    "      print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "      sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "      # Test Accuracy and Prediction\n",
    "      print(\"Performance on the test data:\")\n",
    "      FLAGS.authorise_gold_label = False\n",
    "      test_logits, test_labels, test_weights = batch_predict_with_a_model(test_data, model, session=sess)\n",
    "      test_acc = sess.run(model.final_accuracy, feed_dict={model.logits_placeholder: test_logits.eval(session=sess), \n",
    "                                                             model.label_placeholder: test_labels.eval(session=sess), \n",
    "                                                             model.weight_placeholder: test_weights.eval(session=sess)})\n",
    "      # Print Test Summary\n",
    "      print(\"Test (\"+str(len(test_data.fileindices))+\") accuracy= {:.6f}\".format(test_acc))\n",
    "      # Writing test predictions and final summaries\n",
    "      test_data.write_prediction_summaries(test_logits, \"model.ckpt.epoch-\"+str(FLAGS.model_to_load), session=sess)\n",
    "\n",
    "######################## Main Function ###########################\n",
    "\n",
    "def main(_):\n",
    "  if FLAGS.exp_mode == \"train\":\n",
    "    train()\n",
    "  else:\n",
    "    test()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-e358579bd0c3>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-e358579bd0c3>\"\u001b[1;36m, line \u001b[1;32m49\u001b[0m\n\u001b[1;33m    with tf.Graph().as_default() and tf.device(FLAGS.use_gpu):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################## Batch Testing a model on some dataset ############\n",
    "\n",
    "def batch_predict_with_a_model(data, model, session=None):\n",
    "\n",
    "    data_logits = []\n",
    "    data_labels = []\n",
    "    data_weights = []\n",
    "  \n",
    "    step = 1\n",
    "    while (step * FLAGS.batch_size) <= len(data.fileindices):\n",
    "    # Get batch data as Numpy Arrays : Without shuffling\n",
    "        batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = data.get_batch(((step-1)*FLAGS.batch_size), (step * FLAGS.batch_size))\n",
    "        batch_logits = session.run(model.logits, feed_dict={model.document_placeholder: batch_docs})\n",
    "    \n",
    "        data_logits.append(batch_logits)\n",
    "        data_labels.append(batch_label)\n",
    "        data_weights.append(batch_weight)\n",
    "    \n",
    "        # Increase step\n",
    "        step += 1\n",
    "\n",
    "    # Check if any data left\n",
    "    if (len(data.fileindices) > ((step-1)*FLAGS.batch_size)):\n",
    "        # Get last batch as Numpy Arrays\n",
    "        batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = data.get_batch(((step-1)*FLAGS.batch_size), len(data.fileindices))\n",
    "        batch_logits = session.run(model.logits, feed_dict={model.document_placeholder: batch_docs})\n",
    "    \n",
    "        data_logits.append(batch_logits)\n",
    "        data_labels.append(batch_label)\n",
    "        data_weights.append(batch_weight)\n",
    "        # print(data_logits) \n",
    "    \n",
    "    # Convert list to tensors\n",
    "    data_logits = tf.concat(0, data_logits)\n",
    "    data_lables = tf.concat(0, data_labels)\n",
    "    data_weights = tf.concat(0, data_weights)\n",
    "    # print(data_logits,data_lables,data_weights)\n",
    "    return data_logits, data_lables, data_weights \n",
    "\n",
    "######################## Training Mode ###########################\n",
    "\n",
    "def train():\n",
    "  \"\"\"\n",
    "  Training Mode: Create a new model and train the network\n",
    "  \"\"\"\n",
    "\n",
    "    # Training: use the tf default graph\n",
    "    with tf.Graph().as_default() and tf.device(FLAGS.use_gpu):\n",
    "\n",
    "        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    \n",
    "        # Start a session\n",
    "        with tf.Session(config = config) as sess:\n",
    "      \n",
    "            ### Prepare data for training\n",
    "            print(\"Prepare vocab dict and read pretrained word embeddings ...\")\n",
    "            vocab_dict, word_embedding_array = DataProcessor().prepare_vocab_embeddingdict()\n",
    "            # vocab_dict contains _PAD and _UNK but not word_embedding_array\n",
    "\n",
    "            print(\"Prepare training data ...\")\n",
    "            train_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"training\")\n",
    "      \n",
    "            print(\"Prepare validation data ...\")\n",
    "            validation_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"validation\")\n",
    "\n",
    "            print(\"Prepare ROUGE reward generator ...\")\n",
    "            rouge_generator = Reward_Generator()\n",
    "\n",
    "        # Create Model with various operations\n",
    "        model = MY_Model(sess, len(vocab_dict)-2)\n",
    "      \n",
    "        # Start training with some pretrained model\n",
    "        start_epoch = 1\n",
    "        # selected_modelpath = FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(start_epoch-1)\n",
    "        # if not (os.path.isfile(selected_modelpath)):\n",
    "        #   print(\"Model not found in checkpoint folder.\")\n",
    "        #   exit(0)\n",
    "        # # Reload saved model and test\n",
    "        # print(\"Reading model parameters from %s\" % selected_modelpath)\n",
    "        # model.saver.restore(sess, selected_modelpath)\n",
    "        # print(\"Model loaded.\")\n",
    "\n",
    "        # Initialize word embedding before training\n",
    "        print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "        sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "        ########### Start (No Mixer) Training : Reinforcement learning ################\n",
    "        # Reward aware training as part of Reward weighted CE ,  \n",
    "        # No Curriculam learning: No annealing, consider full document like in MRT\n",
    "        # Multiple Samples (include gold sample),  No future reward, Similar to MRT\n",
    "        # During training does not use PYROUGE to avoid multiple file rewritings\n",
    "        # Approximate MRT with multiple pre-estimated oracle samples\n",
    "        # June 2017: Use Single sample from multiple oracles\n",
    "        ###############################################################################\n",
    "\n",
    "        print(\"Start Reinforcement Training (single rollout from largest prob mass) ...\")\n",
    "\n",
    "        for epoch in range(start_epoch, FLAGS.train_epoch_wce + 1):\n",
    "            print(\"MRT: Epoch \"+str(epoch))\n",
    "        \n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Reshuffle training document indices\")\n",
    "            train_data.shuffle_fileindices()\n",
    "          \n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Restore Rouge Dict\")\n",
    "            rouge_generator.restore_rouge_dict()\n",
    "          \n",
    "            # Start Batch Training\n",
    "            step = 1\n",
    "            while (step * FLAGS.batch_size) <= len(train_data.fileindices):\n",
    "                # Get batch data as Numpy Arrays\n",
    "                batch_docnames, batch_docs, batch_label, batch_weight, batch_oracle_multiple, batch_reward_multiple = train_data.get_batch(((step-1)*FLAGS.batch_size), (step * FLAGS.batch_size))\n",
    "                # print(batch_docnames)\n",
    "                # print(batch_label[0])\n",
    "                # print(batch_weight[0])\n",
    "                # print(batch_oracle_multiple[0])\n",
    "                # print(batch_reward_multiple[0])\n",
    "                # exit(0)\n",
    "          \n",
    "                # Print the progress \n",
    "                if (step % FLAGS.training_checkpoint)  == 0:\n",
    "                    ce_loss_val, ce_loss_sum, acc_val, acc_sum = sess.run([model.rewardweighted_cross_entropy_loss_multisample, model.rewardweighted_ce_multisample_loss_summary, model.accuracy, model.taccuracy_summary],\n",
    "                                                                          feed_dict={model.document_placeholder: batch_docs, \n",
    "                                                                             model.predicted_multisample_label_placeholder: batch_oracle_multiple, \n",
    "                                                                             model.actual_reward_multisample_placeholder: batch_reward_multiple,\n",
    "                                                                             model.label_placeholder: batch_label,\n",
    "                                                                             model.weight_placeholder: batch_weight})\n",
    "            \n",
    "                    # Print Summary to Tensor Board\n",
    "                    model.summary_writer.add_summary(ce_loss_sum, ((epoch-1)*len(train_data.fileindices)+ step*FLAGS.batch_size))\n",
    "                    model.summary_writer.add_summary(acc_sum, ((epoch-1)*len(train_data.fileindices)+step*FLAGS.batch_size))\n",
    "\n",
    "                    print(\"MRT: Epoch \"+str(epoch)+\" : Covered \" + str(step*FLAGS.batch_size)+\"/\"+str(len(train_data.fileindices)) + \n",
    "                  \" : Minibatch Reward Weighted Multisample CE Loss= {:.6f}\".format(ce_loss_val) + \" : Minibatch training accuracy= {:.6f}\".format(acc_val))\n",
    "\n",
    "                # Run optimizer: optimize policy network \n",
    "                sess.run([model.train_op_policynet_expreward], feed_dict={model.document_placeholder: batch_docs, \n",
    "                                                                    model.predicted_multisample_label_placeholder: batch_oracle_multiple, \n",
    "                                                                    model.actual_reward_multisample_placeholder: batch_reward_multiple,\n",
    "                                                                    model.weight_placeholder: batch_weight})\n",
    "          \n",
    "                # Increase step\n",
    "                step += 1\n",
    "          \n",
    "                # if step == 20:\n",
    "                #   break \n",
    "\n",
    "            # Save Model \n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Saving model after epoch completion\")\n",
    "            checkpoint_path = os.path.join(FLAGS.train_dir, \"model.ckpt.epoch-\"+str(epoch))\n",
    "            model.saver.save(sess, checkpoint_path)\n",
    "        \n",
    "            # Backup Rouge Dict\n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Saving rouge dictionary\")\n",
    "            rouge_generator.save_rouge_dict()\n",
    "        \n",
    "            # Performance on the validation set \n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Performance on the validation data\")\n",
    "            # Get Predictions: Prohibit the use of gold labels\n",
    "            validation_logits, validation_labels, validation_weights = batch_predict_with_a_model(validation_data, model, session=sess)\n",
    "            # Validation Accuracy and Prediction\n",
    "            validation_acc, validation_sum = sess.run([model.final_accuracy, model.vaccuracy_summary], feed_dict={model.logits_placeholder: validation_logits.eval(session=sess), \n",
    "                                                                                                              model.label_placeholder: validation_labels.eval(session=sess), \n",
    "                                                                                                              model.weight_placeholder: validation_weights.eval(session=sess)})\n",
    "            # Print Validation Summary \n",
    "            model.summary_writer.add_summary(validation_sum, (epoch*len(train_data.fileindices)))\n",
    "        \n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Validation (\"+str(len(validation_data.fileindices))+\") accuracy= {:.6f}\".format(validation_acc))\n",
    "            # Writing validation predictions and final summaries\n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Writing final validation summaries\")\n",
    "            validation_data.write_prediction_summaries(validation_logits, \"model.ckpt.epoch-\"+str(epoch), session=sess)\n",
    "            # Extimate Rouge Scores\n",
    "            rouge_score = rouge_generator.get_full_rouge(FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(epoch)+\".validation-summary-topranked\", \"validation\")\n",
    "            print(\"MRT: Epoch \"+str(epoch)+\" : Validation (\"+str(len(validation_data.fileindices))+\") rouge= {:.6f}\".format(rouge_score))\n",
    "        \n",
    "            # break\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "# ######################## Test Mode ###########################\n",
    "\n",
    "def test():\n",
    "  \"\"\"\n",
    "  Test Mode: Loads an existing model and test it on the test set\n",
    "  \"\"\"\n",
    "  \n",
    "    # Training: use the tf default graph\n",
    "\n",
    "    with tf.Graph().as_default() and tf.device(FLAGS.use_gpu):\n",
    "\n",
    "        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "        # Start a session\n",
    "        with tf.Session(config = config) as sess:\n",
    "           \n",
    "            ### Prepare data for training\n",
    "            print(\"Prepare vocab dict and read pretrained word embeddings ...\")\n",
    "            vocab_dict, word_embedding_array = DataProcessor().prepare_vocab_embeddingdict()\n",
    "            # vocab_dict contains _PAD and _UNK but not word_embedding_array\n",
    "      \n",
    "            print(\"Prepare test data ...\")\n",
    "            test_data = DataProcessor().prepare_news_data(vocab_dict, data_type=\"test\")\n",
    "\n",
    "            # Create Model with various operations\n",
    "            model = MY_Model(sess, len(vocab_dict)-2)\n",
    "\n",
    "            # # Initialize word embedding before training\n",
    "            # print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "            # sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "            # Select the model\n",
    "            if (os.path.isfile(FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(FLAGS.model_to_load))):\n",
    "                selected_modelpath = FLAGS.train_dir+\"/model.ckpt.epoch-\"+str(FLAGS.model_to_load)\n",
    "            else:\n",
    "                print(\"Model not found in checkpoint folder.\")\n",
    "                exit(0)\n",
    "      \n",
    "            # Reload saved model and test\n",
    "            print(\"Reading model parameters from %s\" % selected_modelpath)\n",
    "            model.saver.restore(sess, selected_modelpath)\n",
    "            print(\"Model loaded.\")\n",
    "\n",
    "            # Initialize word embedding before training\n",
    "            print(\"Initialize word embedding vocabulary with pretrained embeddings ...\")\n",
    "            sess.run(model.vocab_embed_variable.assign(word_embedding_array))\n",
    "\n",
    "            # Test Accuracy and Prediction\n",
    "            print(\"Performance on the test data:\")\n",
    "            FLAGS.authorise_gold_label = False\n",
    "            test_logits, test_labels, test_weights = batch_predict_with_a_model(test_data, model, session=sess)\n",
    "            test_acc = sess.run(model.final_accuracy, feed_dict={model.logits_placeholder: test_logits.eval(session=sess), \n",
    "                                                             model.label_placeholder: test_labels.eval(session=sess), \n",
    "                                                             model.weight_placeholder: test_weights.eval(session=sess)})\n",
    "            # Print Test Summary\n",
    "            print(\"Test (\"+str(len(test_data.fileindices))+\") accuracy= {:.6f}\".format(test_acc))\n",
    "            # Writing test predictions and final summaries\n",
    "            test_data.write_prediction_summaries(test_logits, \"model.ckpt.epoch-\"+str(FLAGS.model_to_load), session=sess)\n",
    "\n",
    "######################## Main Function ###########################\n",
    "\n",
    "def main(_):\n",
    "    if FLAGS.exp_mode == \"train\":\n",
    "        train()\n",
    "    else:\n",
    "        test()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
